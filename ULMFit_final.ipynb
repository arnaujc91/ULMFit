{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running the notebook in COLAB run the following line of code\n",
    "# !pip install fastai==2.0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUseful links:\\nData preprocessing: https://www.kaggle.com/parthsharma5795/comprehensive-twitter-airline-sentiment-analysis\\nTrain ULMFit in IMDB: https://course.fast.ai/videos/?lesson=8\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Useful links:\n",
    "Data preprocessing: https://www.kaggle.com/parthsharma5795/comprehensive-twitter-airline-sentiment-analysis\n",
    "Train ULMFit in IMDB: https://course.fast.ai/videos/?lesson=8\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/arnaujc91/ULMFit/master/data/Tweets.csv'\n",
    "tweets = pd.read_csv(url)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir Thanks! He is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir thx for nothing on getting us out of the country and back to US. Broken plane? Come on. Get another one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>“@AmericanAir: @TilleyMonsta George, that doesn't look good. Please follow this link to start the refund process: http://t.co/4gr39s91Dl”😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir my flight was Cancelled Flightled, leaving tomorrow morning. Auto rebooked for a Tuesday night flight but need to arrive Monday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir right on cue with the delays👌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir thank you we got on a different flight to Chicago.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir Please bring American Airlines to #BlackBerry10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(tweets.to_html(columns=['text'], index=False,header=None, max_rows=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a377fc290>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets.airline_sentiment_confidence.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.626913\n",
       "neutral     0.211680\n",
       "positive    0.161407\n",
       "Name: airline_sentiment, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.airline_sentiment.value_counts()/len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Customer Service Issue                      12\n",
       "Late Flight                                  4\n",
       "Cancelled Flight                             3\n",
       "Can't Tell                                   3\n",
       "Cancelled Flight\\nCustomer Service Issue     2\n",
       "Late Flight\\nFlight Attendant Complaints     1\n",
       "Customer Service Issue\\nLost Luggage         1\n",
       "Late Flight\\nCancelled Flight                1\n",
       "Flight Attendant Complaints                  1\n",
       "Lost Luggage\\nDamaged Luggage                1\n",
       "Late Flight\\nLost Luggage                    1\n",
       "Bad Flight                                   1\n",
       "Customer Service Issue\\nCan't Tell           1\n",
       "Name: negativereason_gold, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.negativereason_gold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you downloaded the file and have it in a \"data\" directory,\\n        uncomment and run the lines below'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''If you downloaded the file and have it in a \"data\" directory,\n",
    "        uncomment and run the lines below'''\n",
    "# data_directory = Path(os.getcwd())/'data'\n",
    "# assert data_directory.is_dir(), 'Data directory not found'\n",
    "# data_files = listdir(data_directory)\n",
    "# print(data_files)\n",
    "\n",
    "# csv_datafile = data_directory/'Tweets.csv'\n",
    "# print('Data file:', csv_datafile)\n",
    "# tweets = pd.read_csv(csv_datafile)\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tweet starts with a [Twitter handle](https://sproutsocial.com/glossary/twitter-handle/) which refers to the Airline, to which the twitter message is adressed to. E.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@VirginAmerica What @dhepburn said.',\n",
       "  \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       "  \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
       "  '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
       "  \"@VirginAmerica and it's a really big bad thing about it\"],\n",
       " ['Virgin America',\n",
       "  'Virgin America',\n",
       "  'Virgin America',\n",
       "  'Virgin America',\n",
       "  'Virgin America'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tweets.text[:5]), list(tweets.airline[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove this information. In order to do so we add a new rule to the [default rules](https://github.com/fastai/fastai/blob/a8ed5a64f93df9be02eef907ddbc355f3ad130d1/fastai/text/core.py#L96) for preprocessing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_first_handle(t):\n",
    "    return re.sub(r'^@\\w* ', '', t)\n",
    "\n",
    "rules = defaults.text_proc_rules\n",
    "rules.insert(0, rm_first_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tokenize the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16033\n",
      "['xxbos', 'xxmaj', 'what', '@dhepburn', 'said']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaujc/miniconda3/envs/fastai2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "tokenized_df, vocab_count = tokenize_df(tweets,  text_cols='text', tok=SpacyTokenizer(), rules = rules)\n",
    "vocab = list(vocab_count.keys())\n",
    "print(len(vocab))\n",
    "print(vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*tokenize_df* returns the tokenized dataframe and also the counting of the tokenized words in the dataset. For example if we want to get all the words that appear at least 3 times, we can write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4683"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([key for key in vocab_count if vocab_count[key]>2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 4683 words appear more often than twice in the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the tokenized VS the original tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  @VirginAmerica What @dhepburn said.\n",
      "after xxbos xxmaj what @dhepburn said .\n",
      "\n",
      "\n",
      "before:  @VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "after xxbos plus you 've added commercials to the experience … tacky .\n",
      "\n",
      "\n",
      "before:  @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      "after xxbos i did n't today … xxmaj must mean i need to take another trip !\n",
      "\n",
      "\n",
      "before:  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "after xxbos it 's really aggressive to blast obnoxious \" entertainment \" in your guests ' faces & & they have little recourse\n",
      "\n",
      "\n",
      "before:  @VirginAmerica and it's a really big bad thing about it\n",
      "after xxbos and it 's a really big bad thing about it\n",
      "\n",
      "\n",
      "before:  @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "after xxbos seriously would pay $ 30 a flight for seats that did n't have this playing . \n",
      " it 's really the only bad thing about flying xxup va\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a,b in zip(list(tokenized_df.text), list(tweets.text)):\n",
    "    print('before: ', b)\n",
    "    print('after: ', ' '.join(a))\n",
    "    print('\\n')\n",
    "    if i == 5:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a [TextDataLoader](https://docs.fast.ai/text.data#TextDataLoaders) which is a Wrapper around the [DataLoader](https://docs.fast.ai/data.core.html#DataLoaders) class. The DataLoader splits our dataset between training and validation. The TextDataLoader adds more functionality specific to NLP problems, like the vocabulary of the data.\n",
    "\n",
    "Now a few things:\n",
    "- Remember that the TextDataLoaders just consider a word as part of the vocab if it appears **more than 3 times** in the entire dataset by default if you use `TextDataLoaders.from_df`.\n",
    "- We also need `is_lm=True` because we will first train a **Language Model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaujc/miniconda3/envs/fastai2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj thank you ! xxbos xxmaj understood . xxmaj thanks anyway xxbos make sure you make xxmaj cancelled xxmaj flighted flight baggage and upgrade fees seem like a xxunk so that you might be able to xxunk a few $ more xxbos i will be calling someone on xxmaj monday in customer relations . xxmaj very disappointed in how i was treated by customer service ! xxbos mechanical failure is n't</td>\n",
       "      <td>xxmaj thank you ! xxbos xxmaj understood . xxmaj thanks anyway xxbos make sure you make xxmaj cancelled xxmaj flighted flight baggage and upgrade fees seem like a xxunk so that you might be able to xxunk a few $ more xxbos i will be calling someone on xxmaj monday in customer relations . xxmaj very disappointed in how i was treated by customer service ! xxbos mechanical failure is n't xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flights and missed connections on first class flights and not get any compensation for losing those seats … xxbos tell them to get on that , please . xxbos xxmaj yeah i have ! xxmaj i 'm on it ! ! xxmaj looking for specific dates - i will just keep checking daily for a fare drop xxbos tough night , two 90 minute calls , on hold , delayed here in</td>\n",
       "      <td>and missed connections on first class flights and not get any compensation for losing those seats … xxbos tell them to get on that , please . xxbos xxmaj yeah i have ! xxmaj i 'm on it ! ! xxmaj looking for specific dates - i will just keep checking daily for a fare drop xxbos tough night , two 90 minute calls , on hold , delayed here in xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my xxup us xxmaj airways number and each time it does n't pick it up xxbos res chg online / app no work , still on hold , faster for hubby to xxup drive to airport to make change @ counter # fail xxbos i got a flight at xxunk on xxmaj thursday but looking for something tomorrow , anything available ? xxbos xxup flt 3 xxrep 3 4 delayed because of</td>\n",
       "      <td>xxup us xxmaj airways number and each time it does n't pick it up xxbos res chg online / app no work , still on hold , faster for hubby to xxup drive to airport to make change @ counter # fail xxbos i got a flight at xxunk on xxmaj thursday but looking for something tomorrow , anything available ? xxbos xxup flt 3 xxrep 3 4 delayed because of maintenance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was given a credit for that automatically . : / xxbos xxmaj twitter says i ca n't xxup dm someone unless they follow me . xxmaj can @southwestair follows my twitter ? thanks you . xxbos - xxmaj group 2 line gets longer every week . xxmaj almost no one left for xxmaj groups 3 - 5 anymore . xxmaj time to make xxmaj explorer xxmaj card xxmaj group 3 ?</td>\n",
       "      <td>was given a credit for that automatically . : / xxbos xxmaj twitter says i ca n't xxup dm someone unless they follow me . xxmaj can @southwestair follows my twitter ? thanks you . xxbos - xxmaj group 2 line gets longer every week . xxmaj almost no one left for xxmaj groups 3 - 5 anymore . xxmaj time to make xxmaj explorer xxmaj card xxmaj group 3 ? xxbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos @virginamerica , i submitted a status match request a while back and still have n’t heard ! xxmaj i ’m flying on xxmaj monday . xxmaj can you look / xxunk ? xxbos xxmaj what 's up with the wait times on your customer service line ? xxmaj tried 2x on xxmaj xxunk . and now xxmaj i 've been on for over 15 min . xxbos i have a xxup</td>\n",
       "      <td>@virginamerica , i submitted a status match request a while back and still have n’t heard ! xxmaj i ’m flying on xxmaj monday . xxmaj can you look / xxunk ? xxbos xxmaj what 's up with the wait times on your customer service line ? xxmaj tried 2x on xxmaj xxunk . and now xxmaj i 've been on for over 15 min . xxbos i have a xxup lh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dls = TextDataLoaders.from_df(tweets, text_col='text',  is_lm=True)\n",
    "lm_dls.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a little bit the structure of this dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__getitem__': 'Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)',\n",
       " 'train': 'Training `DataLoader`',\n",
       " 'valid': 'Validation `DataLoader`',\n",
       " 'train_ds': 'Training `Dataset`',\n",
       " 'valid_ds': 'Validation `Dataset`',\n",
       " 'to': 'Use `device`',\n",
       " 'cuda': 'Use the gpu if available',\n",
       " 'cpu': 'Use the cpu',\n",
       " 'new_empty': 'Create a new empty version of `self` with the same transforms',\n",
       " 'from_dblock': 'Create a dataloaders from a given `dblock`'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dls._docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous DataLoader was for training or fine tunning the language model. The following one will be the DataLoader used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaujc/miniconda3/envs/fastai2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj hi have a question re future xxmaj flight xxmaj booking xxmaj problems . xxup dub - jac 29 / 9 xxup jac - lax 8 / 10 xxup lax - dub 13 / 10 . xxmaj i 'm * xxunk xxmaj what is checked bag allowance for xxup jac - lax ?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxup where xxup is xxup my xxup fucking xxup bag ? ! ? ! xxmaj where the fuck is my fucking bag xxrep 3 ? xxup tell xxup me xxup now xxup or xxup give xxup me a xxup number xxup to xxup call a xxup human . xxup xxunk m xxpad xxpad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos - xxup seriously it 's 2015 ? ! ? ! xxup no wifi on a 5hr flight from xxup cle - sfo # xxunk . xxmaj you 're the xxup only airline w / out wifi … and pls no ' xxunk ' xxup bs . xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxup you xxup are xxup amazing xxrep 3 ! xxup you xxup are xxup the xxup best xxrep 3 ! xxup follow xxup me xxup please xxup and i xxup follow xxup you xxup back ; ) xxrep 3 🙏 ✌ ️ 😉 ) xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj can i put sun in my carry on ? xxup rt “ @united : xxunk xxrep 3 e xxmaj right now 0 would be a heat xxunk , so enjoy the warmth ! xxmaj can you bring some home ? xxunk ” xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxup by xxup the xxup grace xxup of xxup god , i xxup made xxup it ! “ @usairways : xxunk _ _ xxmaj we do n't have those xxunk in here . xxmaj we hope you can make that flight . ” xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj united xxmaj club team is xxup a+ &amp; &amp; got me a seat xxmaj late xxmaj flightr . xxmaj still , not sure why a last min xxup ual xxmaj cancelled xxmaj flightlation costs me $ yet overbooked folks get $ ? xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj all flts to xxup jfk xxmaj cancelled xxmaj flightled xxmaj thx to xxup ur agent at xxup sfo xxmaj i m rebooked on xxup ua . xxmaj did n't get name . xxmaj she was awesome ! # xxunk # xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tc_dls = TextDataLoaders.from_df(tweets, text_col='text', label_col='airline_sentiment')\n",
    "tc_dls.show_batch(max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code of `TextDataLoaders.from_df` we can see that the vocab is created with words that appear **at least 3 times** in the entire dataset. Any words that appear with a lower frequency will be automatically tokenized as `xxunk`, which stands for *unknown*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the words in the vocab are words that appear at least 3 times in the entire dataset, if we want to change that, we can not do it directly from the high level API that fastai offers, but instead we need a couple of more lines of code. Anyway the following lines are just what `TextDataLoaders.from_df` does but changing the parameter **min_freq** to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaujc/miniconda3/envs/fastai2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "If you wanna have a TextDataLoader with words that appear less than 3 times in the dataset:\n",
    "'''\n",
    "# we set min_freq to ONE to allow any words that appear at least once.\n",
    "min_freq=1\n",
    "\n",
    "dblock = DataBlock(blocks=[TextBlock.from_df(text_cols='text', is_lm=True, min_freq=min_freq) ],\n",
    "                           get_x=ColReader(\"text\"),\n",
    "                           splitter=splitter = RandomSplitter(valid_pct=0.2))\n",
    "\n",
    "tweets_f1 = TextDataLoaders.from_dblock(dblock, tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue is that eventhough `TextDataLoaders.from_df` call `tokenize_df`, the first will add some extra special tokens that are not provided by `tokenize_df`. Let's see what this means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary obtained from: \n",
      "\n",
      "   - tokenize_df: 16033\n",
      "\n",
      "   - TextDataLoaders.from_df (min_freq=3): 4688\n",
      "\n",
      "   - TextDataLoaders.from_dblock (min_freq=1): 16040\n"
     ]
    }
   ],
   "source": [
    "print('Length of vocabulary obtained from: ')\n",
    "print(f'\\n   - tokenize_df: {len(set(vocab))}')\n",
    "print(f'\\n   - TextDataLoaders.from_df (min_freq=3): {len(lm_dls.vocab)}')\n",
    "print(f'\\n   - TextDataLoaders.from_dblock (min_freq=1): {len(tweets_f1.vocab)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori you would expect that the vocab from `tokenize_df`and `TextDataLoaders.from_dblock` to be the same size as both obtain the vocab from any word that appears in the dataset. Despite of this one has 7 more items than the other. Which are those items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_diff(vocab1, vocab2):\n",
    "    if len(vocab1)>len(vocab2):\n",
    "        b_vocab = set(vocab1)\n",
    "        s_vocab = set(vocab2)\n",
    "    else:\n",
    "        b_vocab = set(vocab2)\n",
    "        s_vocab = set(vocab1)\n",
    "        \n",
    "    return list(b_vocab-s_vocab.intersection(b_vocab))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added special tokens from `TextDataLoaders.from_df` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xxeos', 'xxfake', 'xxfld']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_diff(tweets_f1.vocab, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, for some reason the special token `xxfake` appears twice in the vocab from `tweets_f1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xxfake': 3}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print({item:count for item, count in collections.Counter(tweets_f1.vocab).items() if count > 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xxeos', 'xxfake', 'xxfld']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_diff([key for key in vocab_to_index if vocab_to_index[key]>2], lm_dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tools for debugging the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful tool for debugging can be to find some word in the original texts, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2279                                                                                                            @united I see. Thanks for explaining.\n",
       "14225    @AmericanAir AA agent said I repeated myself when I was explaining. I told him \"I understand English.\" His reply?\"Our conversation is over.\"\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.text[tweets.text.str.contains('explaining', regex=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"*explaining*\" appears twice in the dataset, once in row 2279 and once in row 14225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful tool is to decode the numericalized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = lm_dls.train_ds.decode(lm_dls.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorText([   2,   25,  725,   30, 1336,   72,   24,    0, 1282,   16,   23,    0,\n",
       "            23,    0,   52,   31,   14,   14,   48,   14,    0]),),\n",
       " ('xxbos is helping me step up my xxunk game ! # xxunk # xxunk http : / / t.co / xxunk',))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dls.train_ds[0], decoded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary between tokenized words and integers is found inside the [Numericalize](https://docs.fast.ai/text.data#Numericalize) class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'xxunk': 0,\n",
       "             'xxpad': 1,\n",
       "             'xxbos': 2,\n",
       "             'xxeos': 3,\n",
       "             'xxfld': 4,\n",
       "             'xxrep': 5,\n",
       "             'xxwrep': 6,\n",
       "             'xxup': 7,\n",
       "             'xxmaj': 8,\n",
       "             '.': 9,\n",
       "             'to': 10,\n",
       "             'i': 11,\n",
       "             'the': 12,\n",
       "             'a': 13,\n",
       "             '/': 14,\n",
       "             'you': 15,\n",
       "             '!': 16,\n",
       "             '?': 17,\n",
       "             ',': 18,\n",
       "             'for': 19,\n",
       "             'flight': 20,\n",
       "             'on': 21,\n",
       "             'and': 22,\n",
       "             '#': 23,\n",
       "             'my': 24,\n",
       "             'is': 25,\n",
       "             'in': 26,\n",
       "             'it': 27,\n",
       "             'of': 28,\n",
       "             \"n't\": 29,\n",
       "             'me': 30,\n",
       "             ':': 31,\n",
       "             'have': 32,\n",
       "             'that': 33,\n",
       "             'your': 34,\n",
       "             'was': 35,\n",
       "             'not': 36,\n",
       "             'with': 37,\n",
       "             '-': 38,\n",
       "             \"'s\": 39,\n",
       "             'no': 40,\n",
       "             'at': 41,\n",
       "             'do': 42,\n",
       "             'this': 43,\n",
       "             'get': 44,\n",
       "             '&': 45,\n",
       "             'we': 46,\n",
       "             'but': 47,\n",
       "             't.co': 48,\n",
       "             'be': 49,\n",
       "             'from': 50,\n",
       "             'can': 51,\n",
       "             'http': 52,\n",
       "             'are': 53,\n",
       "             '…': 54,\n",
       "             'thanks': 55,\n",
       "             'cancelled': 56,\n",
       "             'now': 57,\n",
       "             'an': 58,\n",
       "             'just': 59,\n",
       "             'service': 60,\n",
       "             '3': 61,\n",
       "             '\"': 62,\n",
       "             'so': 63,\n",
       "             'help': 64,\n",
       "             'been': 65,\n",
       "             '2': 66,\n",
       "             'what': 67,\n",
       "             'time': 68,\n",
       "             'will': 69,\n",
       "             'they': 70,\n",
       "             'customer': 71,\n",
       "             'up': 72,\n",
       "             'out': 73,\n",
       "             'all': 74,\n",
       "             'our': 75,\n",
       "             \"'m\": 76,\n",
       "             'us': 77,\n",
       "             'hours': 78,\n",
       "             'when': 79,\n",
       "             'flights': 80,\n",
       "             'how': 81,\n",
       "             'hold': 82,\n",
       "             'there': 83,\n",
       "             'plane': 84,\n",
       "             'if': 85,\n",
       "             'would': 86,\n",
       "             'why': 87,\n",
       "             'thank': 88,\n",
       "             'ca': 89,\n",
       "             'still': 90,\n",
       "             'one': 91,\n",
       "             'please': 92,\n",
       "             'need': 93,\n",
       "             'delayed': 94,\n",
       "             'did': 95,\n",
       "             'back': 96,\n",
       "             'gate': 97,\n",
       "             'had': 98,\n",
       "             'call': 99,\n",
       "             'about': 100,\n",
       "             'has': 101,\n",
       "             'am': 102,\n",
       "             'flightled': 103,\n",
       "             'or': 104,\n",
       "             'bag': 105,\n",
       "             'as': 106,\n",
       "             '4': 107,\n",
       "             'got': 108,\n",
       "             'after': 109,\n",
       "             'hour': 110,\n",
       "             \"'ve\": 111,\n",
       "             ')': 112,\n",
       "             'any': 113,\n",
       "             'like': 114,\n",
       "             'phone': 115,\n",
       "             'late': 116,\n",
       "             'today': 117,\n",
       "             '$': 118,\n",
       "             'airline': 119,\n",
       "             'over': 120,\n",
       "             'more': 121,\n",
       "             '(': 122,\n",
       "             'by': 123,\n",
       "             'again': 124,\n",
       "             'fly': 125,\n",
       "             'guys': 126,\n",
       "             'should': 127,\n",
       "             'waiting': 128,\n",
       "             'does': 129,\n",
       "             'know': 130,\n",
       "             'could': 131,\n",
       "             'way': 132,\n",
       "             'trying': 133,\n",
       "             'airport': 134,\n",
       "             'only': 135,\n",
       "             'great': 136,\n",
       "             'then': 137,\n",
       "             'u': 138,\n",
       "             'day': 139,\n",
       "             'going': 140,\n",
       "             'wait': 141,\n",
       "             'because': 142,\n",
       "             'never': 143,\n",
       "             'flying': 144,\n",
       "             'change': 145,\n",
       "             'weather': 146,\n",
       "             'make': 147,\n",
       "             'go': 148,\n",
       "             'tomorrow': 149,\n",
       "             'last': 150,\n",
       "             \"'re\": 151,\n",
       "             'check': 152,\n",
       "             'delay': 153,\n",
       "             'really': 154,\n",
       "             'good': 155,\n",
       "             '1': 156,\n",
       "             'were': 157,\n",
       "             'even': 158,\n",
       "             'home': 159,\n",
       "             'off': 160,\n",
       "             'minutes': 161,\n",
       "             'here': 162,\n",
       "             'people': 163,\n",
       "             'seat': 164,\n",
       "             'bags': 165,\n",
       "             'another': 166,\n",
       "             '@jetblue': 167,\n",
       "             'new': 168,\n",
       "             'want': 169,\n",
       "             'told': 170,\n",
       "             'w': 171,\n",
       "             'first': 172,\n",
       "             'united': 173,\n",
       "             'take': 174,\n",
       "             'very': 175,\n",
       "             '..': 176,\n",
       "             'agent': 177,\n",
       "             'see': 178,\n",
       "             '@americanair': 179,\n",
       "             'luggage': 180,\n",
       "             'dm': 181,\n",
       "             '5': 182,\n",
       "             'ticket': 183,\n",
       "             'them': 184,\n",
       "             'she': 185,\n",
       "             'than': 186,\n",
       "             'ever': 187,\n",
       "             'getting': 188,\n",
       "             'love': 189,\n",
       "             'lost': 190,\n",
       "             'due': 191,\n",
       "             \"'ll\": 192,\n",
       "             'number': 193,\n",
       "             'too': 194,\n",
       "             'travel': 195,\n",
       "             'worst': 196,\n",
       "             'someone': 197,\n",
       "             'yes': 198,\n",
       "             'work': 199,\n",
       "             'next': 200,\n",
       "             'email': 201,\n",
       "             'let': 202,\n",
       "             'much': 203,\n",
       "             'crew': 204,\n",
       "             'baggage': 205,\n",
       "             'hrs': 206,\n",
       "             'who': 207,\n",
       "             'flighted': 208,\n",
       "             'days': 209,\n",
       "             'two': 210,\n",
       "             'through': 211,\n",
       "             'aa': 212,\n",
       "             'made': 213,\n",
       "             'trip': 214,\n",
       "             'before': 215,\n",
       "             'right': 216,\n",
       "             'their': 217,\n",
       "             'seats': 218,\n",
       "             'other': 219,\n",
       "             'response': 220,\n",
       "             'being': 221,\n",
       "             'experience': 222,\n",
       "             'problems': 223,\n",
       "             'online': 224,\n",
       "             'some': 225,\n",
       "             'jfk': 226,\n",
       "             'sitting': 227,\n",
       "             'best': 228,\n",
       "             '@united': 229,\n",
       "             'her': 230,\n",
       "             'staff': 231,\n",
       "             'sent': 232,\n",
       "             '“': 233,\n",
       "             '”': 234,\n",
       "             'where': 235,\n",
       "             'passengers': 236,\n",
       "             'bad': 237,\n",
       "             'wo': 238,\n",
       "             'min': 239,\n",
       "             'better': 240,\n",
       "             'customers': 241,\n",
       "             'boarding': 242,\n",
       "             'long': 243,\n",
       "             'already': 244,\n",
       "             'line': 245,\n",
       "             'book': 246,\n",
       "             'booked': 247,\n",
       "             'said': 248,\n",
       "             'sure': 249,\n",
       "             'left': 250,\n",
       "             '6': 251,\n",
       "             'since': 252,\n",
       "             'he': 253,\n",
       "             'times': 254,\n",
       "             'stuck': 255,\n",
       "             'give': 256,\n",
       "             'well': 257,\n",
       "             'morning': 258,\n",
       "             'reservation': 259,\n",
       "             'dfw': 260,\n",
       "             'into': 261,\n",
       "             '+': 262,\n",
       "             'miles': 263,\n",
       "             'tonight': 264,\n",
       "             'flightr': 265,\n",
       "             'miss': 266,\n",
       "             'same': 267,\n",
       "             'think': 268,\n",
       "             'website': 269,\n",
       "             'find': 270,\n",
       "             'agents': 271,\n",
       "             'yet': 272,\n",
       "             'nothing': 273,\n",
       "             'airlines': 274,\n",
       "             'issue': 275,\n",
       "             '10': 276,\n",
       "             'use': 277,\n",
       "             'care': 278,\n",
       "             'connection': 279,\n",
       "             'fleet': 280,\n",
       "             'fleek': 281,\n",
       "             'lax': 282,\n",
       "             'says': 283,\n",
       "             'booking': 284,\n",
       "             'delays': 285,\n",
       "             'night': 286,\n",
       "             'nt': 287,\n",
       "             'hotel': 288,\n",
       "             'refund': 289,\n",
       "             'jetblue': 290,\n",
       "             'system': 291,\n",
       "             'tried': 292,\n",
       "             'flt': 293,\n",
       "             'tell': 294,\n",
       "             'air': 295,\n",
       "             'info': 296,\n",
       "             ':)': 297,\n",
       "             'put': 298,\n",
       "             'rebooked': 299,\n",
       "             'keep': 300,\n",
       "             'hope': 301,\n",
       "             '@usairways': 302,\n",
       "             'anything': 303,\n",
       "             'pay': 304,\n",
       "             'called': 305,\n",
       "             'nice': 306,\n",
       "             'rude': 307,\n",
       "             'missed': 308,\n",
       "             'down': 309,\n",
       "             'ago': 310,\n",
       "             'also': 311,\n",
       "             'free': 312,\n",
       "             'follow': 313,\n",
       "             \"'\": 314,\n",
       "             'its': 315,\n",
       "             'done': 316,\n",
       "             '30': 317,\n",
       "             'tickets': 318,\n",
       "             'class': 319,\n",
       "             'finally': 320,\n",
       "             'awesome': 321,\n",
       "             \"'d\": 322,\n",
       "             'mins': 323,\n",
       "             'rebook': 324,\n",
       "             '7': 325,\n",
       "             'phl': 326,\n",
       "             'issues': 327,\n",
       "             'able': 328,\n",
       "             'week': 329,\n",
       "             'sfo': 330,\n",
       "             'credit': 331,\n",
       "             '8': 332,\n",
       "             'checked': 333,\n",
       "             'wifi': 334,\n",
       "             'ord': 335,\n",
       "             'every': 336,\n",
       "             'business': 337,\n",
       "             'working': 338,\n",
       "             '️': 339,\n",
       "             'until': 340,\n",
       "             'anyone': 341,\n",
       "             'appreciate': 342,\n",
       "             'having': 343,\n",
       "             'come': 344,\n",
       "             'say': 345,\n",
       "             'missing': 346,\n",
       "             'status': 347,\n",
       "             'looking': 348,\n",
       "             'board': 349,\n",
       "             'app': 350,\n",
       "             'team': 351,\n",
       "             'available': 352,\n",
       "             'via': 353,\n",
       "             'dca': 354,\n",
       "             'person': 355,\n",
       "             'update': 356,\n",
       "             'problem': 357,\n",
       "             'voucher': 358,\n",
       "             'always': 359,\n",
       "             'which': 360,\n",
       "             'yesterday': 361,\n",
       "             'ok': 362,\n",
       "             'without': 363,\n",
       "             'making': 364,\n",
       "             'thx': 365,\n",
       "             'leave': 366,\n",
       "             'answer': 367,\n",
       "             'many': 368,\n",
       "             '\\n': 369,\n",
       "             'upgrade': 370,\n",
       "             'sorry': 371,\n",
       "             'helpful': 372,\n",
       "             'terrible': 373,\n",
       "             'understand': 374,\n",
       "             '20': 375,\n",
       "             'look': 376,\n",
       "             'early': 377,\n",
       "             'fail': 378,\n",
       "             'disappointed': 379,\n",
       "             'extra': 380,\n",
       "             'instead': 381,\n",
       "             'pm': 382,\n",
       "             'amazing': 383,\n",
       "             '1st': 384,\n",
       "             'clt': 385,\n",
       "             'ewr': 386,\n",
       "             'name': 387,\n",
       "             'year': 388,\n",
       "             'speak': 389,\n",
       "             'open': 390,\n",
       "             'stop': 391,\n",
       "             'rep': 392,\n",
       "             'fix': 393,\n",
       "             'money': 394,\n",
       "             'send': 395,\n",
       "             'hi': 396,\n",
       "             'paid': 397,\n",
       "             'claim': 398,\n",
       "             'planes': 399,\n",
       "             'almost': 400,\n",
       "             'contact': 401,\n",
       "             'bos': 402,\n",
       "             'rt': 403,\n",
       "             'though': 404,\n",
       "             'earlier': 405,\n",
       "             'pilot': 406,\n",
       "             'pass': 407,\n",
       "             'm': 408,\n",
       "             'southwest': 409,\n",
       "             'departure': 410,\n",
       "             'site': 411,\n",
       "             'dallas': 412,\n",
       "             'happy': 413,\n",
       "             'took': 414,\n",
       "             'different': 415,\n",
       "             '@southwestair': 416,\n",
       "             'connecting': 417,\n",
       "             'boston': 418,\n",
       "             'while': 419,\n",
       "             'try': 420,\n",
       "             'talk': 421,\n",
       "             'supposed': 422,\n",
       "             'vegas': 423,\n",
       "             'hung': 424,\n",
       "             'attendant': 425,\n",
       "             'full': 426,\n",
       "             ':(': 427,\n",
       "             'unacceptable': 428,\n",
       "             'received': 429,\n",
       "             'something': 430,\n",
       "             'ridiculous': 431,\n",
       "             'least': 432,\n",
       "             'direct': 433,\n",
       "             'poor': 434,\n",
       "             'employees': 435,\n",
       "             'everyone': 436,\n",
       "             'airways': 437,\n",
       "             'wife': 438,\n",
       "             'job': 439,\n",
       "             'actually': 440,\n",
       "             'oh': 441,\n",
       "             'company': 442,\n",
       "             'stranded': 443,\n",
       "             'family': 444,\n",
       "             'ground': 445,\n",
       "             'chicago': 446,\n",
       "             ';': 447,\n",
       "             'horrible': 448,\n",
       "             'show': 449,\n",
       "             'old': 450,\n",
       "             '15': 451,\n",
       "             'tarmac': 452,\n",
       "             'card': 453,\n",
       "             'denver': 454,\n",
       "             'his': 455,\n",
       "             'destinationdragons': 456,\n",
       "             'add': 457,\n",
       "             'twitter': 458,\n",
       "             '24': 459,\n",
       "             'found': 460,\n",
       "             'seriously': 461,\n",
       "             'san': 462,\n",
       "             'calling': 463,\n",
       "             'snow': 464,\n",
       "             'those': 465,\n",
       "             'start': 466,\n",
       "             'most': 467,\n",
       "             'landed': 468,\n",
       "             'taking': 469,\n",
       "             'ur': 470,\n",
       "             'food': 471,\n",
       "             'leaving': 472,\n",
       "             'needs': 473,\n",
       "             'chance': 474,\n",
       "             'doing': 475,\n",
       "             'maybe': 476,\n",
       "             'american': 477,\n",
       "             'far': 478,\n",
       "             'weeks': 479,\n",
       "             'wrong': 480,\n",
       "             'tweet': 481,\n",
       "             '45': 482,\n",
       "             'away': 483,\n",
       "             'policy': 484,\n",
       "             'reason': 485,\n",
       "             'might': 486,\n",
       "             'point': 487,\n",
       "             '@': 488,\n",
       "             'vacation': 489,\n",
       "             'gave': 490,\n",
       "             'account': 491,\n",
       "             '0': 492,\n",
       "             'reply': 493,\n",
       "             'half': 494,\n",
       "             'fee': 495,\n",
       "             'both': 496,\n",
       "             'enough': 497,\n",
       "             'message': 498,\n",
       "             'big': 499,\n",
       "             'sit': 500,\n",
       "             '40': 501,\n",
       "             'coming': 502,\n",
       "             'o': 503,\n",
       "             'reservations': 504,\n",
       "             'return': 505,\n",
       "             'option': 506,\n",
       "             'hey': 507,\n",
       "             'once': 508,\n",
       "             'b': 509,\n",
       "             'desk': 510,\n",
       "             'confirmation': 511,\n",
       "             'past': 512,\n",
       "             'charge': 513,\n",
       "             'hr': 514,\n",
       "             'thing': 515,\n",
       "             'nyc': 516,\n",
       "             'may': 517,\n",
       "             'three': 518,\n",
       "             'went': 519,\n",
       "             'frustrated': 520,\n",
       "             'mechanical': 521,\n",
       "             'bc': 522,\n",
       "             'little': 523,\n",
       "             'together': 524,\n",
       "             'broken': 525,\n",
       "             'car': 526,\n",
       "             'these': 527,\n",
       "             '@delta': 528,\n",
       "             'using': 529,\n",
       "             'changed': 530,\n",
       "             'link': 531,\n",
       "             'offer': 532,\n",
       "             'question': 533,\n",
       "             'kids': 534,\n",
       "             'asked': 535,\n",
       "             'twice': 536,\n",
       "             're': 537,\n",
       "             'soon': 538,\n",
       "             '9': 539,\n",
       "             'destination': 540,\n",
       "             '>': 541,\n",
       "             'iad': 542,\n",
       "             'around': 543,\n",
       "             '50': 544,\n",
       "             'him': 545,\n",
       "             'newark': 546,\n",
       "             'less': 547,\n",
       "             'charlotte': 548,\n",
       "             'ua': 549,\n",
       "             'cool': 550,\n",
       "             'possible': 551,\n",
       "             'stay': 552,\n",
       "             'saying': 553,\n",
       "             'used': 554,\n",
       "             '25': 555,\n",
       "             'things': 556,\n",
       "             's': 557,\n",
       "             'looks': 558,\n",
       "             'pls': 559,\n",
       "             'makes': 560,\n",
       "             'few': 561,\n",
       "             'phx': 562,\n",
       "             'longer': 563,\n",
       "             'lot': 564,\n",
       "             'guess': 565,\n",
       "             'houston': 566,\n",
       "             'runway': 567,\n",
       "             'fll': 568,\n",
       "             'worse': 569,\n",
       "             'cost': 570,\n",
       "             '800': 571,\n",
       "             'traveling': 572,\n",
       "             'real': 573,\n",
       "             'friend': 574,\n",
       "             'landing': 575,\n",
       "             'given': 576,\n",
       "             'else': 577,\n",
       "             'awful': 578,\n",
       "             'terminal': 579,\n",
       "             'during': 580,\n",
       "             'scheduled': 581,\n",
       "             'plus': 582,\n",
       "             'hard': 583,\n",
       "             'points': 584,\n",
       "             'waited': 585,\n",
       "             'lga': 586,\n",
       "             'telling': 587,\n",
       "             'idea': 588,\n",
       "             '✈': 589,\n",
       "             'y': 590,\n",
       "             'life': 591,\n",
       "             'seems': 592,\n",
       "             'lack': 593,\n",
       "             'hear': 594,\n",
       "             'frustrating': 595,\n",
       "             '200': 596,\n",
       "             'mean': 597,\n",
       "             'https': 598,\n",
       "             'arrived': 599,\n",
       "             'such': 600,\n",
       "             'heard': 601,\n",
       "             'happened': 602,\n",
       "             'row': 603,\n",
       "             'sucks': 604,\n",
       "             'own': 605,\n",
       "             'thru': 606,\n",
       "             'attendants': 607,\n",
       "             'believe': 608,\n",
       "             'sat': 609,\n",
       "             'cust': 610,\n",
       "             'lol': 611,\n",
       "             'philly': 612,\n",
       "             'international': 613,\n",
       "             'feel': 614,\n",
       "             'everything': 615,\n",
       "             'member': 616,\n",
       "             'assistance': 617,\n",
       "             'hoping': 618,\n",
       "             'calls': 619,\n",
       "             'iah': 620,\n",
       "             'arrive': 621,\n",
       "             'd': 622,\n",
       "             'options': 623,\n",
       "             'end': 624,\n",
       "             '100': 625,\n",
       "             'monday': 626,\n",
       "             'carry': 627,\n",
       "             'request': 628,\n",
       "             'miami': 629,\n",
       "             'information': 630,\n",
       "             'den': 631,\n",
       "             'reflight': 632,\n",
       "             'maintenance': 633,\n",
       "             'swa': 634,\n",
       "             'dc': 635,\n",
       "             'minute': 636,\n",
       "             'quick': 637,\n",
       "             'loyal': 638,\n",
       "             'pick': 639,\n",
       "             'usairways': 640,\n",
       "             'second': 641,\n",
       "             'las': 642,\n",
       "             'expect': 643,\n",
       "             'standby': 644,\n",
       "             'glad': 645,\n",
       "             'jet': 646,\n",
       "             'changes': 647,\n",
       "             'counting': 648,\n",
       "             'process': 649,\n",
       "             'forward': 650,\n",
       "             'wanted': 651,\n",
       "             'ceo': 652,\n",
       "             'each': 653,\n",
       "             'bwi': 654,\n",
       "             'n’t': 655,\n",
       "             'error': 656,\n",
       "             'joke': 657,\n",
       "             'ask': 658,\n",
       "             'busy': 659,\n",
       "             'non': 660,\n",
       "             '--': 661,\n",
       "             'world': 662,\n",
       "             'checking': 663,\n",
       "             'thought': 664,\n",
       "             'room': 665,\n",
       "             '11': 666,\n",
       "             'needed': 667,\n",
       "             'spent': 668,\n",
       "             'usair': 669,\n",
       "             '2nd': 670,\n",
       "             'wish': 671,\n",
       "             'la': 672,\n",
       "             'c': 673,\n",
       "             'suck': 674,\n",
       "             'pilots': 675,\n",
       "             'boarded': 676,\n",
       "             'case': 677,\n",
       "             'reach': 678,\n",
       "             'apology': 679,\n",
       "             'complaint': 680,\n",
       "             'blue': 681,\n",
       "             'deal': 682,\n",
       "             'city': 683,\n",
       "             'wow': 684,\n",
       "             'human': 685,\n",
       "             'bring': 686,\n",
       "             'tv': 687,\n",
       "             'paying': 688,\n",
       "             'guy': 689,\n",
       "             'years': 690,\n",
       "             'empty': 691,\n",
       "             'computer': 692,\n",
       "             'group': 693,\n",
       "             'fees': 694,\n",
       "             'asking': 695,\n",
       "             'communication': 696,\n",
       "             'sw': 697,\n",
       "             'flew': 698,\n",
       "             'club': 699,\n",
       "             'yeah': 700,\n",
       "             'buy': 701,\n",
       "             'lose': 702,\n",
       "             'passenger': 703,\n",
       "             'entire': 704,\n",
       "             'address': 705,\n",
       "             'between': 706,\n",
       "             'form': 707,\n",
       "             'unitedairlines': 708,\n",
       "             'sunday': 709,\n",
       "             '@imaginedragons': 710,\n",
       "             'award': 711,\n",
       "             'bought': 712,\n",
       "             'provide': 713,\n",
       "             'respond': 714,\n",
       "             'place': 715,\n",
       "             'currently': 716,\n",
       "             \"y'\": 717,\n",
       "             'flighting': 718,\n",
       "             'run': 719,\n",
       "             'taken': 720,\n",
       "             'pretty': 721,\n",
       "             'happens': 722,\n",
       "             'under': 723,\n",
       "             'updates': 724,\n",
       "             'helping': 725,\n",
       "             '12': 726,\n",
       "             'whole': 727,\n",
       "             'months': 728,\n",
       "             'drive': 729,\n",
       "             'leg': 730,\n",
       "             'counter': 731,\n",
       "             'hopefully': 732,\n",
       "             'either': 733,\n",
       "             'asap': 734,\n",
       "             'month': 735,\n",
       "             '=': 736,\n",
       "             'supervisor': 737,\n",
       "             'rather': 738,\n",
       "             'delta': 739,\n",
       "             'seem': 740,\n",
       "             'happen': 741,\n",
       "             'list': 742,\n",
       "             'live': 743,\n",
       "             'beyond': 744,\n",
       "             'moved': 745,\n",
       "             'correct': 746,\n",
       "             'super': 747,\n",
       "             'r': 748,\n",
       "             'confirmed': 749,\n",
       "             'wtf': 750,\n",
       "             'price': 751,\n",
       "             'future': 752,\n",
       "             'sleep': 753,\n",
       "             'fault': 754,\n",
       "             'nashville': 755,\n",
       "             'flightlations': 756,\n",
       "             'appreciated': 757,\n",
       "             '*': 758,\n",
       "             'companion': 759,\n",
       "             'holding': 760,\n",
       "             'fine': 761,\n",
       "             'tsa': 762,\n",
       "             'high': 763,\n",
       "             'ready': 764,\n",
       "             'date': 765,\n",
       "             'atl': 766,\n",
       "             'automated': 767,\n",
       "             'mco': 768,\n",
       "             'support': 769,\n",
       "             'plans': 770,\n",
       "             'austin': 771,\n",
       "             'safety': 772,\n",
       "             'details': 773,\n",
       "             'upset': 774,\n",
       "             'okay': 775,\n",
       "             'figure': 776,\n",
       "             'media': 777,\n",
       "             'sad': 778,\n",
       "             'offered': 779,\n",
       "             'checkin': 780,\n",
       "             'apparently': 781,\n",
       "             'access': 782,\n",
       "             'easy': 783,\n",
       "             'cabin': 784,\n",
       "             'purchase': 785,\n",
       "             'orlando': 786,\n",
       "             'top': 787,\n",
       "             'situation': 788,\n",
       "             'tuesday': 789,\n",
       "             'hate': 790,\n",
       "             'atlanta': 791,\n",
       "             'customerservice': 792,\n",
       "             'flown': 793,\n",
       "             '2015': 794,\n",
       "             'completely': 795,\n",
       "             'reschedule': 796,\n",
       "             'luv': 797,\n",
       "             'handle': 798,\n",
       "             'na': 799,\n",
       "             'treat': 800,\n",
       "             'probably': 801,\n",
       "             'set': 802,\n",
       "             'part': 803,\n",
       "             'compensation': 804,\n",
       "             'employee': 805,\n",
       "             '1.5': 806,\n",
       "             'following': 807,\n",
       "             'record': 808,\n",
       "             'must': 809,\n",
       "             'cause': 810,\n",
       "             '👍': 811,\n",
       "             'friends': 812,\n",
       "             'running': 813,\n",
       "             ';)': 814,\n",
       "             'charged': 815,\n",
       "             'n': 816,\n",
       "             'e': 817,\n",
       "             'luck': 818,\n",
       "             'storm': 819,\n",
       "             'read': 820,\n",
       "             'zero': 821,\n",
       "             'clothes': 822,\n",
       "             'original': 823,\n",
       "             'delivered': 824,\n",
       "             'means': 825,\n",
       "             'gets': 826,\n",
       "             'sending': 827,\n",
       "             'switch': 828,\n",
       "             'disconnected': 829,\n",
       "             'kind': 830,\n",
       "             'cold': 831,\n",
       "             'middle': 832,\n",
       "             'feb': 833,\n",
       "             'fare': 834,\n",
       "             'connections': 835,\n",
       "             'platinum': 836,\n",
       "             'members': 837,\n",
       "             'fact': 838,\n",
       "             'ice': 839,\n",
       "             'seen': 840,\n",
       "             'allowed': 841,\n",
       "             'huge': 842,\n",
       "             'losing': 843,\n",
       "             'allow': 844,\n",
       "             'schedule': 845,\n",
       "             'share': 846,\n",
       "             'front': 847,\n",
       "             'giving': 848,\n",
       "             'priority': 849,\n",
       "             'gon': 850,\n",
       "             '😂': 851,\n",
       "             'fun': 852,\n",
       "             'gives': 853,\n",
       "             'helped': 854,\n",
       "             'several': 855,\n",
       "             'myself': 856,\n",
       "             'rdu': 857,\n",
       "             'changing': 858,\n",
       "             'mia': 859,\n",
       "             'answering': 860,\n",
       "             'four': 861,\n",
       "             'friendly': 862,\n",
       "             'pre': 863,\n",
       "             'came': 864,\n",
       "             'despite': 865,\n",
       "             'space': 866,\n",
       "             '1k': 867,\n",
       "             'shit': 868,\n",
       "             'explain': 869,\n",
       "             'aircraft': 870,\n",
       "             'weekend': 871,\n",
       "             'extremely': 872,\n",
       "             'crazy': 873,\n",
       "             'hello': 874,\n",
       "             'country': 875,\n",
       "             'rock': 876,\n",
       "             'layover': 877,\n",
       "             'lots': 878,\n",
       "             'takes': 879,\n",
       "             'route': 880,\n",
       "             'gold': 881,\n",
       "             'flightlation': 882,\n",
       "             'husband': 883,\n",
       "             'round': 884,\n",
       "             'spoke': 885,\n",
       "             'word': 886,\n",
       "             'ppl': 887,\n",
       "             'services': 888,\n",
       "             'winter': 889,\n",
       "             'lounge': 890,\n",
       "             'fixed': 891,\n",
       "             'unable': 892,\n",
       "             'multiple': 893,\n",
       "             'sense': 894,\n",
       "             'water': 895,\n",
       "             '@virginamerica': 896,\n",
       "             'birthday': 897,\n",
       "             'mobile': 898,\n",
       "             'friday': 899,\n",
       "             '😭': 900,\n",
       "             'rescheduled': 901,\n",
       "             'land': 902,\n",
       "             'inconvenience': 903,\n",
       "             'mileage': 904,\n",
       "             'tix': 905,\n",
       "             'complete': 906,\n",
       "             'reps': 907,\n",
       "             'knew': 908,\n",
       "             'goes': 909,\n",
       "             '.@united': 910,\n",
       "             'held': 911,\n",
       "             'works': 912,\n",
       "             'arrival': 913,\n",
       "             'answers': 914,\n",
       "             'unfortunately': 915,\n",
       "             'control': 916,\n",
       "             'bna': 917,\n",
       "             'wall': 918,\n",
       "             'totally': 919,\n",
       "             'afternoon': 920,\n",
       "             'report': 921,\n",
       "             'absolutely': 922,\n",
       "             'overnight': 923,\n",
       "             'mom': 924,\n",
       "             'mine': 925,\n",
       "             'kudos': 926,\n",
       "             'load': 927,\n",
       "             'confirm': 928,\n",
       "             'nope': 929,\n",
       "             'responding': 930,\n",
       "             'phones': 931,\n",
       "             'treated': 932,\n",
       "             'hang': 933,\n",
       "             'plan': 934,\n",
       "             'true': 935,\n",
       "             'relations': 936,\n",
       "             'haha': 937,\n",
       "             'virgin': 938,\n",
       "             '❤': 939,\n",
       "             'news': 940,\n",
       "             '😊': 941,\n",
       "             'code': 942,\n",
       "             'folks': 943,\n",
       "             'keeps': 944,\n",
       "             'spend': 945,\n",
       "             'area': 946,\n",
       "             'small': 947,\n",
       "             '3rd': 948,\n",
       "             'child': 949,\n",
       "             'learn': 950,\n",
       "             'overhead': 951,\n",
       "             '90': 952,\n",
       "             'wonderful': 953,\n",
       "             'baby': 954,\n",
       "             'drop': 955,\n",
       "             'worth': 956,\n",
       "             'literally': 957,\n",
       "             'dealing': 958,\n",
       "             'showing': 959,\n",
       "             'attitude': 960,\n",
       "             'social': 961,\n",
       "             'letter': 962,\n",
       "             'dividend': 963,\n",
       "             'page': 964,\n",
       "             'anyway': 965,\n",
       "             'closed': 966,\n",
       "             'ny': 967,\n",
       "             'bit': 968,\n",
       "             'march': 969,\n",
       "             'evening': 970,\n",
       "             'course': 971,\n",
       "             'excellent': 972,\n",
       "             'man': 973,\n",
       "             'saw': 974,\n",
       "             'short': 975,\n",
       "             'daughter': 976,\n",
       "             'vouchers': 977,\n",
       "             'connect': 978,\n",
       "             '🙏': 979,\n",
       "             'offering': 980,\n",
       "             'fl': 981,\n",
       "             'excited': 982,\n",
       "             'sign': 983,\n",
       "             'flyer': 984,\n",
       "             'ride': 985,\n",
       "             'watch': 986,\n",
       "             '%': 987,\n",
       "             'depart': 988,\n",
       "             'choice': 989,\n",
       "             'americanairlines': 990,\n",
       "             'passbook': 991,\n",
       "             ':-)': 992,\n",
       "             'yr': 993,\n",
       "             'submitted': 994,\n",
       "             'others': 995,\n",
       "             'anymore': 996,\n",
       "             'arriving': 997,\n",
       "             'matter': 998,\n",
       "             'conf': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dls.train_ds.numericalize.o2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [lm_dls.train_ds.numericalize.o2i[word] for word in decoded[0][0].split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for text in decoded:\n",
    "        if 'xxeos' in text[0]:\n",
    "            print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the [Callbacks](https://docs.fast.ai/callback.tracker) we are going to use:\n",
    "- [ActivationStats](https://docs.fast.ai/callback.hook#ActivationStats): Callback that record the mean and std of activations.\n",
    "- [ShowGraphCallback](https://docs.fast.ai/callback.progress#ShowGraphCallback): Update a graph of training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [\n",
    "       ShowGraphCallback,\n",
    "       ActivationStats(with_hist=True),\n",
    "       SaveModelCallback\n",
    "#        ParamScheduler(sched)\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the learner. The [Learner](https://docs.fast.ai/learner) class is the class that contains everything necessary for training. It contains:\n",
    "- DataLoaders\n",
    "- Model\n",
    "- Loss function\n",
    "- Optimizer \n",
    "- Splitter to split the model in several parameter groups\n",
    "- Callbacks for the training.\n",
    "- etc.\n",
    "\n",
    "In the following line we will pass as arguments to the function [language_model_learner](https://docs.fast.ai/text.learner#language_model_learner): \n",
    "- The DataLoader\n",
    "- The name of the model: [AWS_LSTM](https://docs.fast.ai/text.models.awdlstm)\n",
    "- The Callbacks\n",
    "- The path were we wanna save the trained model or part of the model, e.g. just the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = language_model_learner(lm_dls, AWD_LSTM, cbs=cbs, path=Path(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are already certain Callbacks which are set up by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TrainEvalCallback,\n",
       " Recorder,\n",
       " ActivationStats,\n",
       " SaveModelCallback,\n",
       " ModelResetter,\n",
       " RNNRegularizer,\n",
       " GatherPredsCallback,\n",
       " ProgressCallback]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(learner.cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will do the same for the Learner of the Text Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(tc_dls, AWD_LSTM, drop_mult=0.5, cbs=cbs, metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=1.318256749982538e-07, lr_steep=9.12010818865383e-07)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's follow the receipt from Jeremy Howard in his [paper](https://arxiv.org/abs/1801.06146). It is important to understand whats the difference between `fit`and `fit_one_cycle` (for that take a look [here](https://iconof.com/1cycle-learning-rate-policy/)). Also as you will see we will progressively unfreeze the layers during training, this is seen to perform better than just `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learner.fit_one_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, 2e-2)\n",
    "learner.save('language_model')\n",
    "learner.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')\n",
    "learn.fit_one_cycle(12,  2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFINING 1\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(5, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFINING 2\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(3, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFINING 3\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ShowGraphCallback Callback, it crashes with learn.predict\n",
    "learner.remove_cb(ShowGraphCallback)\n",
    "learn.remove_cb(ShowGraphCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test the language model; you can use it to create inventend sentences and see if they make sense. The more the invented sentences look like made by a person the better trained the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.load('language_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For whatever reason this callback interferes with the learner.predict method, giving an error.\n",
    "learner.remove_cb(ShowGraphCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'horrible flight experience with status match . the Exec Platinum'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(\"horrible flight\", 10, temperature=0.75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know if we are getting a good performance, we can compare our model training to a benchmark. We can use the IMDB dataset from fastai and see if we get a similar performance or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "imdb = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaujc/miniconda3/envs/fastai2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaujc/miniconda3/envs/fastai2/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "imdb_cls  = TextDataLoaders.from_df(imdb, text_col='text', label_col='label')\n",
    "imdb_lm = TextDataLoaders.from_df(imdb, text_col='text', is_lm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare how similar are the features of both datasets, for example we can check how many words does any review contain compared to how many words do the tweets contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5f4df850>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.concat([tweets.text[:1000].apply(lambda s: len(s.split())),\n",
    "           imdb.text.apply(lambda s: len(s.split()))],\n",
    "           axis=1,\n",
    "           keys=['tweets', 'imdb']).plot.hist(alpha=0.4, bins = 500, xlim=(0,400)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5e2ad5d0>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets.text.apply(lambda s: len(s.split())).hist(bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5adefd50>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb.text.apply(lambda s: len(s.split())).hist(bins=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the reviews are in general much bigger than the tweets, something that is already expected but can influence the performance of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to analyze is the vocabs, is the vocab from `imdb` much bigger than from the `tweets`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4688, 7080)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dls.vocab), len(imdb_lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the vocab of `imdb` is almost the double as the vocab for the `tweets`, therefore it could be that the language model for the `imdb` performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify things, I will just define a function that does everything we have done so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_training(lm_dls, cl_dls, cbs=None):\n",
    "\n",
    "    if cbs==None:\n",
    "        cbs = [\n",
    "              ShowGraphCallback,\n",
    "              ActivationStats,\n",
    "              SaveModelCallback\n",
    "            ]\n",
    "\n",
    "    learner = language_model_learner(lm_dls, AWD_LSTM, cbs = cbs,  metrics=[accuracy])\n",
    "    # TRY CHANGING drop_mult, to see if there is an effect in training small datasets\n",
    "    learn = text_classifier_learner(cl_dls, AWD_LSTM, drop_mult=0.5, cbs=cbs, metrics=accuracy).to_fp16()\n",
    "\n",
    "    # ----  TRAIN THE LANGUAGE MODEL  ----\n",
    "    learner.fit_one_cycle(10, 2e-2)\n",
    "    # learner.save('language_model')\n",
    "    learner.save_encoder('finetuned')\n",
    "\n",
    "    # ----  TRAIN THE CLASSIFIER  ----\n",
    "    learn = learn.load_encoder('finetuned')\n",
    "    learn.fit_one_cycle(12,  2e-3)\n",
    "\n",
    "    # REFINING 1\n",
    "    learn.freeze_to(-2)\n",
    "    learn.fit_one_cycle(5, slice(1e-2/(2.6**4),1e-2))\n",
    "\n",
    "    # REFINING 2\n",
    "    learn.freeze_to(-3)\n",
    "    learn.fit_one_cycle(3, slice(5e-3/(2.6**4),5e-3))\n",
    "\n",
    "    # REFINING 3\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n",
    "\n",
    "    learner.remove_cb(ShowGraphCallback)\n",
    "    learn.remove_cb(ShowGraphCallback)\n",
    "\n",
    "    return learner, learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_imdb, learn_imdb = complete_training(imdb_lm, imdb_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 34, 353, 223,  ..., 204,  97, 246],\n",
       "        [371, 355,  64,  ..., 210, 394, 190],\n",
       "        [268,  82, 281,  ..., 370, 350, 348],\n",
       "        ...,\n",
       "        [ 56, 309, 146,  ...,  43, 213, 121],\n",
       "        [ 88, 364, 168,  ...,  75, 320, 198],\n",
       "        [327, 242, 151,  ...,  60, 224, 179]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(400, (64,72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 72])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.one_batch()[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "{'mean': -1.0784971714019775, 'std': 1.7559617757797241, 'near_zero': 0.7773613187474692, 'hist': tensor([6.4589e+05, 5.5320e+05, 4.6831e+05, 3.9165e+05, 3.2837e+05, 2.7821e+05,\n",
      "        2.3211e+05, 1.9375e+05, 1.6353e+05, 1.3667e+05, 1.1454e+05, 9.6002e+04,\n",
      "        7.9578e+04, 6.7210e+04, 5.5756e+04, 4.6225e+04, 3.8507e+04, 3.2020e+04,\n",
      "        2.7027e+04, 2.2465e+04, 1.8770e+04, 1.5887e+04, 1.3319e+04, 1.1015e+04,\n",
      "        9.1940e+03, 7.5300e+03, 6.1130e+03, 5.1200e+03, 4.1790e+03, 3.5160e+03,\n",
      "        2.6490e+03, 2.2620e+03, 1.7800e+03, 1.4300e+03, 1.0650e+03, 7.9300e+02,\n",
      "        6.1300e+02, 5.2900e+02, 3.8500e+02, 2.8100e+02])}\n"
     ]
    }
   ],
   "source": [
    "for hook in learner.activation_stats.hooks.hooks:\n",
    "    print(hook.stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for hook in learner.activation_stats.hooks.hooks:\n",
    "    print(hook.removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ActivationStats.hook of ActivationStats>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.activation_stats.hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: False, 1: False, 2: False, 3: True}, {0: 4, 1: 4, 2: 4, 3: 2})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_pg = {}\n",
    "layers_pg = {}\n",
    "for n, group in enumerate(learner.opt.param_lists):\n",
    "    trainable_pg[n]= group[0].requires_grad\n",
    "    layers_pg[n] = len(group)\n",
    "trainable_pg, layers_pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-solved Issue with Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far it seems that the defautl `AWD_LSTM` arquitecture from fastai is not really working well with hooks. Let's work it out in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "  print(f\"Working for layer: -- {m._get_name()} --\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in modules:\n",
    "    m.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(4688, 400, padding_idx=1),\n",
       " Embedding(4688, 400, padding_idx=1),\n",
       " LSTM(400, 1152, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(1152, 1152, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(1152, 400, batch_first=True),\n",
       " ParameterModule(),\n",
       " Linear(in_features=400, out_features=4688, bias=True)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = [m for m in flatten_model(learner.model) if has_params(m)]; modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "awd(\n",
       "  (encoder): Embedding(3, 5, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(3, 5, padding_idx=1)\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): LSTM(5, 6, batch_first=True)\n",
       "    (1): LSTM(6, 5, batch_first=True)\n",
       "  )\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = get_language_model(AWD_LSTM, 100)\n",
    "model =  awd(vocab_sz=3,\n",
    "                  emb_sz=5,\n",
    "                  n_hid=6,\n",
    "                  n_layers=2)\n",
    "model.encoder.register_forward_hook(hook_fn)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "Embedding\n",
      "LSTM\n",
      "LSTM\n",
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- LSTM --\n",
      "Working for layer: -- LSTM --\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0587,  0.1035,  0.0819, -0.0410, -0.1014],\n",
       "         [ 0.1026,  0.1546,  0.1246, -0.0702, -0.1392],\n",
       "         [ 0.1310,  0.1815,  0.1482, -0.0887, -0.1515],\n",
       "         [ 0.1484,  0.1964,  0.1615, -0.1002, -0.1549]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for m in flatten_model(model):\n",
    "    if has_params(m):\n",
    "        print(m._get_name())\n",
    "        m.register_forward_hook(hook_fn)\n",
    "# print(model.bs ==  torch.randint(3, (1,4)).shape[0])\n",
    "model.reset()\n",
    "model(torch.randint(3, (1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0745,  0.0718, -0.1414, -0.1269, -0.0425],\n",
       "         [-0.1317,  0.0577, -0.1877, -0.2021, -0.0905],\n",
       "         [-0.1637,  0.0457, -0.2116, -0.2390, -0.1234],\n",
       "         [-0.1816,  0.0393, -0.2249, -0.2561, -0.1437]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randint(3, (1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "??to_detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(3, 5, padding_idx=1),\n",
       " Embedding(3, 5, padding_idx=1),\n",
       " LSTM(5, 6, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(6, 5, batch_first=True),\n",
       " ParameterModule()]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in flatten_model(model) if has_params(m)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT IS THE DROPOUT!!!! IT IS NOT WORKING WITH THE HOOKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class awd(Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token=1, hidden_p=0.2, input_p=0.6, embed_p=0.1,\n",
    "                 weight_p=0.5, bidir=False):\n",
    "        store_attr('emb_sz,n_hid,n_layers,pad_token')\n",
    "        self.bs = 1\n",
    "        self.n_dir = 2 if bidir else 1\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir,\n",
    "                                                 bidir, weight_p, l) for l in range(n_layers)])\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, inp, from_embeds=False):\n",
    "        bs,sl = inp.shape\n",
    "\n",
    "#         output = self.encoder (inp)\n",
    "        output = self.input_dp(self.encoder(inp))\n",
    "#        AVOIDS FORWARDS_HOOKS FOR EMBBED -> It is the encoder_dp\n",
    "#         output = self.input_dp(self.encoder_dp(inp))\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            output, _ = rnn(output, self.hidden[l])\n",
    "            if l != self.n_layers - 1: output = hid_dp(output)\n",
    "        return output\n",
    "\n",
    "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
    "        \"Return one of the inner rnn\"\n",
    "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True)\n",
    "#         AVOIDS FORWARD_HOOKS FOR RNN\n",
    "#         return WeightDropout(rnn, weight_p)\n",
    "        return rnn\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
    "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
    "\n",
    "    def _change_one_hidden(self, l, bs):\n",
    "        if self.bs < bs:\n",
    "            nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
    "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
    "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
    "        return self.hidden[l]\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)\n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(out),raw,out\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "LSTM\n",
      "Linear\n"
     ]
    }
   ],
   "source": [
    "lmmodel = LMModel7(5,2, 1, p=0.1)\n",
    "for m in flatten_model(lmmodel):\n",
    "    if has_params(m):\n",
    "        print(m._get_name())\n",
    "        m.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for layer: -- Embedding --\n",
      "Working for layer: -- LSTM --\n",
      "Working for layer: -- Linear --\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 4.8395e-01, -5.1640e-01,  6.1054e-01, -2.0308e-02, -9.3443e-01],\n",
       "          [ 4.3933e-01, -5.8131e-01,  6.1733e-01, -7.4054e-02, -1.0070e+00],\n",
       "          [ 4.2293e-01, -6.1924e-01,  6.2065e-01, -1.0316e-01, -1.0216e+00],\n",
       "          [ 4.2007e-01, -6.2445e-01,  6.2114e-01, -1.0730e-01, -1.0254e+00],\n",
       "          [ 4.9134e-01, -4.8462e-01,  6.0819e-01,  2.5667e-03, -9.4050e-01]],\n",
       " \n",
       "         [[ 4.8990e-01, -4.8656e-01,  6.0840e-01,  9.3740e-04, -9.4298e-01],\n",
       "          [ 4.3532e-01, -6.0054e-01,  6.1872e-01, -8.7787e-02, -1.0020e+00],\n",
       "          [ 4.3759e-01, -5.8244e-01,  6.1751e-01, -7.5218e-02, -1.0111e+00],\n",
       "          [ 4.2285e-01, -6.1841e-01,  6.2060e-01, -1.0263e-01, -1.0226e+00],\n",
       "          [ 6.6006e-01, -2.0128e-01,  5.8032e-01,  2.3099e-01, -6.9853e-01]],\n",
       " \n",
       "         [[ 6.4582e-01, -2.3694e-01,  5.8336e-01,  2.0391e-01, -7.0887e-01],\n",
       "          [ 4.6919e-01, -5.1824e-01,  6.1164e-01, -2.5042e-02, -9.7535e-01],\n",
       "          [ 4.3655e-01, -5.7787e-01,  6.1732e-01, -7.2433e-02, -1.0180e+00],\n",
       "          [ 4.3465e-01, -5.8497e-01,  6.1786e-01, -7.7602e-02, -1.0174e+00],\n",
       "          [ 4.1859e-01, -6.1786e-01,  6.2086e-01, -1.0327e-01, -1.0353e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 5.5151e-01, -3.4074e-01,  5.9576e-01,  1.1248e-01, -8.9104e-01],\n",
       "          [ 4.3783e-01, -5.9636e-01,  6.1831e-01, -8.4411e-02, -9.9840e-01],\n",
       "          [ 4.3848e-01, -5.8106e-01,  6.1737e-01, -7.4096e-02, -1.0097e+00],\n",
       "          [ 4.3373e-01, -5.9033e-01,  6.1823e-01, -8.1381e-02, -1.0154e+00],\n",
       "          [ 4.9211e-01, -4.8144e-01,  6.0796e-01,  4.8667e-03, -9.4102e-01]],\n",
       " \n",
       "         [[ 5.0128e-01, -4.6075e-01,  6.0613e-01,  2.0792e-02, -9.3242e-01],\n",
       "          [ 4.5968e-01, -5.3444e-01,  6.1323e-01, -3.8070e-02, -9.8879e-01],\n",
       "          [ 4.4418e-01, -5.6480e-01,  6.1604e-01, -6.1935e-02, -1.0073e+00],\n",
       "          [ 4.2312e-01, -6.0758e-01,  6.1995e-01, -9.5367e-02, -1.0311e+00],\n",
       "          [ 4.0826e-01, -6.5352e-01,  6.2363e-01, -1.2943e-01, -1.0344e+00]],\n",
       " \n",
       "         [[ 4.8990e-01, -4.8656e-01,  6.0840e-01,  9.3740e-04, -9.4298e-01],\n",
       "          [ 4.3532e-01, -6.0054e-01,  6.1872e-01, -8.7787e-02, -1.0020e+00],\n",
       "          [ 5.4725e-01, -3.4621e-01,  5.9636e-01,  1.0784e-01, -8.9859e-01],\n",
       "          [ 4.3402e-01, -5.9195e-01,  6.1831e-01, -8.2394e-02, -1.0132e+00],\n",
       "          [ 4.2022e-01, -6.2415e-01,  6.2111e-01, -1.0707e-01, -1.0252e+00]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.1686, -0.1408],\n",
       "          [ 0.2314, -0.1528],\n",
       "          [ 0.2355, -0.1795],\n",
       "          [ 0.2382, -0.1819],\n",
       "          [ 0.1864, -0.1055]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2190, -0.1751],\n",
       "          [ 0.2357, -0.1510],\n",
       "          [ 0.2369, -0.1780],\n",
       "          [ 0.1872, -0.1030]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2157, -0.1136],\n",
       "          [ 0.2453, -0.1417],\n",
       "          [ 0.2422, -0.1491],\n",
       "          [ 0.2520, -0.1685]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.1784, -0.0360],\n",
       "          [ 0.1832, -0.0411],\n",
       "          [ 0.2274, -0.0996],\n",
       "          [ 0.2324, -0.1669]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.1805, -0.0643],\n",
       "          [ 0.2178, -0.1630],\n",
       "          [ 0.2300, -0.1888],\n",
       "          [ 0.2362, -0.1801]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2215, -0.1510],\n",
       "          [ 0.2356, -0.1452],\n",
       "          [ 0.2390, -0.1519],\n",
       "          [ 0.2351, -0.1988]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2272, -0.1098],\n",
       "          [ 0.2323, -0.1734],\n",
       "          [ 0.2397, -0.1493],\n",
       "          [ 0.1879, -0.0988]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2230, -0.1422],\n",
       "          [ 0.1865, -0.0850],\n",
       "          [ 0.1819, -0.0776],\n",
       "          [ 0.2353, -0.1444]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2245, -0.1283],\n",
       "          [ 0.2361, -0.1420],\n",
       "          [ 0.2343, -0.1928],\n",
       "          [ 0.1865, -0.1034]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2259, -0.1199],\n",
       "          [ 0.2471, -0.1525],\n",
       "          [ 0.2428, -0.1555],\n",
       "          [ 0.2414, -0.1574]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2195, -0.0914],\n",
       "          [ 0.1884, -0.0648],\n",
       "          [ 0.2204, -0.1624],\n",
       "          [ 0.1861, -0.0891]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.1805, -0.0643],\n",
       "          [ 0.2346, -0.1351],\n",
       "          [ 0.2344, -0.1882],\n",
       "          [ 0.2398, -0.1566]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.1805, -0.0744],\n",
       "          [ 0.2228, -0.1483],\n",
       "          [ 0.2370, -0.1431],\n",
       "          [ 0.2497, -0.1657]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.1805, -0.0643],\n",
       "          [ 0.2261, -0.1180],\n",
       "          [ 0.2378, -0.1349],\n",
       "          [ 0.2390, -0.1668]],\n",
       " \n",
       "         [[ 0.1686, -0.1408],\n",
       "          [ 0.2314, -0.1528],\n",
       "          [ 0.2314, -0.2009],\n",
       "          [ 0.2492, -0.1770],\n",
       "          [ 0.1877, -0.1157]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.1805, -0.0744],\n",
       "          [ 0.2176, -0.1709],\n",
       "          [ 0.2359, -0.1487],\n",
       "          [ 0.2488, -0.1693]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2268, -0.1303],\n",
       "          [ 0.1871, -0.0874],\n",
       "          [ 0.2198, -0.1805],\n",
       "          [ 0.2363, -0.1532]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2215, -0.1510],\n",
       "          [ 0.1851, -0.0898],\n",
       "          [ 0.2356, -0.1542],\n",
       "          [ 0.2384, -0.1787]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.1787, -0.0724],\n",
       "          [ 0.2220, -0.1471],\n",
       "          [ 0.2368, -0.1425],\n",
       "          [ 0.2379, -0.1721]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2268, -0.1303],\n",
       "          [ 0.2372, -0.1428],\n",
       "          [ 0.2386, -0.1719],\n",
       "          [ 0.1880, -0.0998]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2245, -0.1283],\n",
       "          [ 0.2300, -0.1856],\n",
       "          [ 0.2491, -0.1699],\n",
       "          [ 0.1884, -0.1113]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2268, -0.1303],\n",
       "          [ 0.2312, -0.1863],\n",
       "          [ 0.1863, -0.1004],\n",
       "          [ 0.2187, -0.1920]],\n",
       " \n",
       "         [[ 0.1686, -0.1408],\n",
       "          [ 0.2314, -0.1528],\n",
       "          [ 0.2473, -0.1720],\n",
       "          [ 0.2412, -0.1902],\n",
       "          [ 0.2515, -0.1791]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.1784, -0.0360],\n",
       "          [ 0.2260, -0.0961],\n",
       "          [ 0.2480, -0.1386],\n",
       "          [ 0.1919, -0.0916]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2190, -0.1751],\n",
       "          [ 0.1843, -0.0955],\n",
       "          [ 0.2233, -0.1658],\n",
       "          [ 0.2351, -0.1747]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2346, -0.1367],\n",
       "          [ 0.2497, -0.1618],\n",
       "          [ 0.2433, -0.1612],\n",
       "          [ 0.2414, -0.1610]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2245, -0.1283],\n",
       "          [ 0.2300, -0.1856],\n",
       "          [ 0.1861, -0.1001],\n",
       "          [ 0.1807, -0.0895]],\n",
       " \n",
       "         [[ 0.1686, -0.1408],\n",
       "          [ 0.2196, -0.1606],\n",
       "          [ 0.2327, -0.1732],\n",
       "          [ 0.2328, -0.2003],\n",
       "          [ 0.2384, -0.1631]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2245, -0.1283],\n",
       "          [ 0.2361, -0.1420],\n",
       "          [ 0.2383, -0.1714],\n",
       "          [ 0.2405, -0.1550]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2272, -0.1098],\n",
       "          [ 0.2365, -0.1517],\n",
       "          [ 0.2351, -0.1876],\n",
       "          [ 0.2396, -0.1566]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2272, -0.1098],\n",
       "          [ 0.1892, -0.0752],\n",
       "          [ 0.2373, -0.1426],\n",
       "          [ 0.2352, -0.1926]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.1787, -0.0724],\n",
       "          [ 0.1809, -0.0683],\n",
       "          [ 0.2265, -0.1207],\n",
       "          [ 0.2317, -0.1800]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2346, -0.1367],\n",
       "          [ 0.1886, -0.0909],\n",
       "          [ 0.2253, -0.1609],\n",
       "          [ 0.1863, -0.0941]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2346, -0.1367],\n",
       "          [ 0.2395, -0.1463],\n",
       "          [ 0.2393, -0.1739],\n",
       "          [ 0.2395, -0.1785]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.1787, -0.0724],\n",
       "          [ 0.2168, -0.1698],\n",
       "          [ 0.2295, -0.1923],\n",
       "          [ 0.2478, -0.1736]],\n",
       " \n",
       "         [[ 0.1686, -0.1408],\n",
       "          [ 0.2227, -0.1375],\n",
       "          [ 0.1848, -0.0922],\n",
       "          [ 0.1816, -0.0828],\n",
       "          [ 0.2234, -0.1544]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2268, -0.1303],\n",
       "          [ 0.2354, -0.1648],\n",
       "          [ 0.2398, -0.1516],\n",
       "          [ 0.2505, -0.1705]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2241, -0.1527],\n",
       "          [ 0.2367, -0.1458],\n",
       "          [ 0.2378, -0.1742],\n",
       "          [ 0.2403, -0.1565]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2332, -0.1444],\n",
       "          [ 0.2383, -0.1515],\n",
       "          [ 0.2402, -0.1552],\n",
       "          [ 0.1886, -0.1017]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2272, -0.1098],\n",
       "          [ 0.2384, -0.1301],\n",
       "          [ 0.2511, -0.1576],\n",
       "          [ 0.1911, -0.1029]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2357, -0.1465],\n",
       "          [ 0.2498, -0.1675],\n",
       "          [ 0.2384, -0.2080],\n",
       "          [ 0.2355, -0.2100]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2268, -0.1303],\n",
       "          [ 0.2354, -0.1648],\n",
       "          [ 0.2384, -0.1737],\n",
       "          [ 0.2505, -0.1712]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2357, -0.1465],\n",
       "          [ 0.2498, -0.1675],\n",
       "          [ 0.2421, -0.1870],\n",
       "          [ 0.1877, -0.1076]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2190, -0.1751],\n",
       "          [ 0.2357, -0.1510],\n",
       "          [ 0.2384, -0.1559],\n",
       "          [ 0.2387, -0.1801]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2157, -0.1136],\n",
       "          [ 0.2291, -0.1690],\n",
       "          [ 0.2329, -0.1911],\n",
       "          [ 0.2384, -0.1587]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2179, -0.1648],\n",
       "          [ 0.1849, -0.0906],\n",
       "          [ 0.2264, -0.1390],\n",
       "          [ 0.2357, -0.1697]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2241, -0.1527],\n",
       "          [ 0.2367, -0.1458],\n",
       "          [ 0.2378, -0.1742],\n",
       "          [ 0.2509, -0.1711]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.1805, -0.0744],\n",
       "          [ 0.2345, -0.1426],\n",
       "          [ 0.2398, -0.1495],\n",
       "          [ 0.2353, -0.1972]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2195, -0.0914],\n",
       "          [ 0.2298, -0.1629],\n",
       "          [ 0.2337, -0.1878],\n",
       "          [ 0.2493, -0.1712]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2357, -0.1465],\n",
       "          [ 0.2380, -0.1744],\n",
       "          [ 0.2351, -0.2001],\n",
       "          [ 0.2391, -0.1629]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2195, -0.0914],\n",
       "          [ 0.2298, -0.1629],\n",
       "          [ 0.2337, -0.1878],\n",
       "          [ 0.2375, -0.1793]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2190, -0.1751],\n",
       "          [ 0.2458, -0.1657],\n",
       "          [ 0.2398, -0.1870],\n",
       "          [ 0.2394, -0.1858]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2272, -0.1098],\n",
       "          [ 0.2384, -0.1301],\n",
       "          [ 0.2408, -0.1419],\n",
       "          [ 0.2398, -0.1710]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2230, -0.1422],\n",
       "          [ 0.2307, -0.1838],\n",
       "          [ 0.2370, -0.1773],\n",
       "          [ 0.1862, -0.1033]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2245, -0.1283],\n",
       "          [ 0.1863, -0.0865],\n",
       "          [ 0.2276, -0.1350],\n",
       "          [ 0.2479, -0.1606]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2241, -0.1527],\n",
       "          [ 0.2350, -0.1679],\n",
       "          [ 0.2337, -0.1971],\n",
       "          [ 0.2495, -0.1755]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.1805, -0.0744],\n",
       "          [ 0.2228, -0.1483],\n",
       "          [ 0.2471, -0.1584],\n",
       "          [ 0.2526, -0.1743]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.1805, -0.0643],\n",
       "          [ 0.2261, -0.1180],\n",
       "          [ 0.1883, -0.0798],\n",
       "          [ 0.2202, -0.1744]],\n",
       " \n",
       "         [[ 0.1686, -0.1408],\n",
       "          [ 0.2142, -0.1832],\n",
       "          [ 0.2333, -0.1558],\n",
       "          [ 0.2320, -0.2026],\n",
       "          [ 0.2494, -0.1778]],\n",
       " \n",
       "         [[ 0.1581, -0.0275],\n",
       "          [ 0.2100, -0.1372],\n",
       "          [ 0.2442, -0.1486],\n",
       "          [ 0.2399, -0.1757],\n",
       "          [ 0.2409, -0.1572]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2230, -0.1422],\n",
       "          [ 0.1865, -0.0850],\n",
       "          [ 0.2360, -0.1504],\n",
       "          [ 0.2504, -0.1692]],\n",
       " \n",
       "         [[ 0.1774, -0.1141],\n",
       "          [ 0.2162, -0.1736],\n",
       "          [ 0.2346, -0.1507],\n",
       "          [ 0.2381, -0.1557],\n",
       "          [ 0.1881, -0.1020]],\n",
       " \n",
       "         [[ 0.1851, -0.0879],\n",
       "          [ 0.2259, -0.1199],\n",
       "          [ 0.2372, -0.1365],\n",
       "          [ 0.2505, -0.1614],\n",
       "          [ 0.2388, -0.2040]],\n",
       " \n",
       "         [[ 0.1887, -0.1056],\n",
       "          [ 0.2190, -0.1751],\n",
       "          [ 0.1843, -0.0955],\n",
       "          [ 0.2349, -0.1588],\n",
       "          [ 0.2381, -0.1818]]], grad_fn=<TransposeBackward0>),\n",
       " tensor([[[ 0.1874, -0.1564],\n",
       "          [ 0.2571, -0.1698],\n",
       "          [ 0.2617, -0.1995],\n",
       "          [ 0.2646, -0.2022],\n",
       "          [ 0.2072, -0.1172]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2434, -0.1946],\n",
       "          [ 0.2619, -0.1678],\n",
       "          [ 0.2633, -0.1978],\n",
       "          [ 0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.0306],\n",
       "          [ 0.2397, -0.1263],\n",
       "          [ 0.2726, -0.1574],\n",
       "          [ 0.2691, -0.1656],\n",
       "          [ 0.2800, -0.1872]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.0000, -0.0000],\n",
       "          [ 0.2036, -0.0457],\n",
       "          [ 0.0000, -0.1107],\n",
       "          [ 0.2582, -0.1854]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2006, -0.0715],\n",
       "          [ 0.2420, -0.1811],\n",
       "          [ 0.2555, -0.2098],\n",
       "          [ 0.2625, -0.2001]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2461, -0.1678],\n",
       "          [ 0.2618, -0.1614],\n",
       "          [ 0.0000, -0.1688],\n",
       "          [ 0.2612, -0.2208]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2525, -0.0000],\n",
       "          [ 0.2581, -0.0000],\n",
       "          [ 0.2663, -0.1659],\n",
       "          [ 0.2088, -0.1098]],\n",
       " \n",
       "         [[ 0.2056, -0.0000],\n",
       "          [ 0.2478, -0.1580],\n",
       "          [ 0.2072, -0.0944],\n",
       "          [ 0.2022, -0.0862],\n",
       "          [ 0.2614, -0.1604]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2494, -0.0000],\n",
       "          [ 0.2624, -0.1578],\n",
       "          [ 0.2603, -0.2142],\n",
       "          [ 0.2072, -0.0000]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2510, -0.1333],\n",
       "          [ 0.2746, -0.1694],\n",
       "          [ 0.2698, -0.1727],\n",
       "          [ 0.2683, -0.0000]],\n",
       " \n",
       "         [[ 0.1757, -0.0000],\n",
       "          [ 0.2439, -0.1016],\n",
       "          [ 0.2094, -0.0721],\n",
       "          [ 0.2449, -0.1805],\n",
       "          [ 0.2068, -0.0990]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2006, -0.0715],\n",
       "          [ 0.2607, -0.1502],\n",
       "          [ 0.2604, -0.2091],\n",
       "          [ 0.2664, -0.1740]],\n",
       " \n",
       "         [[ 0.0000, -0.1173],\n",
       "          [ 0.2005, -0.0826],\n",
       "          [ 0.2476, -0.1648],\n",
       "          [ 0.2634, -0.1590],\n",
       "          [ 0.2775, -0.1841]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2006, -0.0715],\n",
       "          [ 0.2513, -0.1311],\n",
       "          [ 0.0000, -0.1499],\n",
       "          [ 0.2655, -0.1853]],\n",
       " \n",
       "         [[ 0.1874, -0.1564],\n",
       "          [ 0.2571, -0.1698],\n",
       "          [ 0.0000, -0.2232],\n",
       "          [ 0.2769, -0.1966],\n",
       "          [ 0.0000, -0.1285]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2005, -0.0826],\n",
       "          [ 0.0000, -0.1899],\n",
       "          [ 0.2621, -0.0000],\n",
       "          [ 0.2764, -0.1881]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2520, -0.0000],\n",
       "          [ 0.2079, -0.0971],\n",
       "          [ 0.2442, -0.2006],\n",
       "          [ 0.2626, -0.1702]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2461, -0.1678],\n",
       "          [ 0.2057, -0.0998],\n",
       "          [ 0.2617, -0.1713],\n",
       "          [ 0.2649, -0.1986]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.1985, -0.0804],\n",
       "          [ 0.2467, -0.1635],\n",
       "          [ 0.2631, -0.1584],\n",
       "          [ 0.2644, -0.1912]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2520, -0.1448],\n",
       "          [ 0.2635, -0.1587],\n",
       "          [ 0.2651, -0.1910],\n",
       "          [ 0.2089, -0.1109]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2494, -0.1426],\n",
       "          [ 0.2556, -0.2062],\n",
       "          [ 0.2768, -0.1888],\n",
       "          [ 0.2093, -0.1236]],\n",
       " \n",
       "         [[ 0.0000, -0.1173],\n",
       "          [ 0.2520, -0.1448],\n",
       "          [ 0.2569, -0.2070],\n",
       "          [ 0.2070, -0.1116],\n",
       "          [ 0.0000, -0.2133]],\n",
       " \n",
       "         [[ 0.1874, -0.0000],\n",
       "          [ 0.2571, -0.1698],\n",
       "          [ 0.2748, -0.1911],\n",
       "          [ 0.0000, -0.2113],\n",
       "          [ 0.2795, -0.1990]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.1983, -0.0400],\n",
       "          [ 0.2512, -0.1068],\n",
       "          [ 0.2755, -0.1540],\n",
       "          [ 0.2132, -0.1017]],\n",
       " \n",
       "         [[ 0.2097, -0.0000],\n",
       "          [ 0.2434, -0.1946],\n",
       "          [ 0.0000, -0.1061],\n",
       "          [ 0.2481, -0.1842],\n",
       "          [ 0.0000, -0.1941]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2607, -0.1519],\n",
       "          [ 0.2774, -0.1797],\n",
       "          [ 0.2703, -0.1791],\n",
       "          [ 0.2682, -0.1789]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2494, -0.1426],\n",
       "          [ 0.2556, -0.2062],\n",
       "          [ 0.2068, -0.1112],\n",
       "          [ 0.2008, -0.0994]],\n",
       " \n",
       "         [[ 0.1874, -0.1564],\n",
       "          [ 0.2440, -0.1785],\n",
       "          [ 0.2585, -0.1924],\n",
       "          [ 0.0000, -0.2226],\n",
       "          [ 0.2649, -0.1812]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2494, -0.1426],\n",
       "          [ 0.2624, -0.1578],\n",
       "          [ 0.2648, -0.1905],\n",
       "          [ 0.2672, -0.1722]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2525, -0.1220],\n",
       "          [ 0.0000, -0.1685],\n",
       "          [ 0.2613, -0.2084],\n",
       "          [ 0.2662, -0.1739]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2525, -0.0000],\n",
       "          [ 0.2102, -0.0836],\n",
       "          [ 0.2637, -0.1585],\n",
       "          [ 0.2613, -0.2140]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.0000, -0.0000],\n",
       "          [ 0.2010, -0.0759],\n",
       "          [ 0.2516, -0.1341],\n",
       "          [ 0.2575, -0.2000]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2607, -0.1519],\n",
       "          [ 0.2096, -0.1010],\n",
       "          [ 0.0000, -0.1788],\n",
       "          [ 0.2070, -0.1045]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2607, -0.1519],\n",
       "          [ 0.2661, -0.1626],\n",
       "          [ 0.2659, -0.1933],\n",
       "          [ 0.2661, -0.1983]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.1985, -0.0804],\n",
       "          [ 0.2408, -0.0000],\n",
       "          [ 0.2551, -0.2136],\n",
       "          [ 0.2753, -0.1929]],\n",
       " \n",
       "         [[ 0.1874, -0.1564],\n",
       "          [ 0.2475, -0.1528],\n",
       "          [ 0.2054, -0.1025],\n",
       "          [ 0.2017, -0.0920],\n",
       "          [ 0.2483, -0.1716]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2520, -0.1448],\n",
       "          [ 0.2616, -0.1831],\n",
       "          [ 0.2665, -0.1684],\n",
       "          [ 0.2783, -0.0000]],\n",
       " \n",
       "         [[ 0.0000, -0.1173],\n",
       "          [ 0.2490, -0.1697],\n",
       "          [ 0.2630, -0.1620],\n",
       "          [ 0.2643, -0.1936],\n",
       "          [ 0.0000, -0.1739]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2591, -0.1604],\n",
       "          [ 0.2648, -0.1684],\n",
       "          [ 0.2669, -0.1725],\n",
       "          [ 0.2095, -0.1130]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2525, -0.1220],\n",
       "          [ 0.2648, -0.1445],\n",
       "          [ 0.2790, -0.1751],\n",
       "          [ 0.2123, -0.1143]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2619, -0.1627],\n",
       "          [ 0.2775, -0.1861],\n",
       "          [ 0.2649, -0.2311],\n",
       "          [ 0.2616, -0.2333]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2520, -0.1448],\n",
       "          [ 0.2616, -0.1831],\n",
       "          [ 0.2649, -0.1930],\n",
       "          [ 0.2783, -0.1902]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2619, -0.1627],\n",
       "          [ 0.2775, -0.1861],\n",
       "          [ 0.2690, -0.2078],\n",
       "          [ 0.2085, -0.1195]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2434, -0.1946],\n",
       "          [ 0.2619, -0.1678],\n",
       "          [ 0.2649, -0.1732],\n",
       "          [ 0.2653, -0.2002]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2397, -0.1263],\n",
       "          [ 0.2545, -0.1877],\n",
       "          [ 0.2587, -0.0000],\n",
       "          [ 0.2649, -0.1763]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2421, -0.0000],\n",
       "          [ 0.2054, -0.1007],\n",
       "          [ 0.2516, -0.1545],\n",
       "          [ 0.2619, -0.1886]],\n",
       " \n",
       "         [[ 0.2097, -0.0000],\n",
       "          [ 0.0000, -0.1697],\n",
       "          [ 0.2630, -0.1620],\n",
       "          [ 0.2643, -0.1936],\n",
       "          [ 0.2788, -0.0000]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2005, -0.0826],\n",
       "          [ 0.2606, -0.1584],\n",
       "          [ 0.2664, -0.1661],\n",
       "          [ 0.2614, -0.2191]],\n",
       " \n",
       "         [[ 0.0000, -0.0306],\n",
       "          [ 0.2439, -0.1016],\n",
       "          [ 0.2553, -0.1810],\n",
       "          [ 0.2596, -0.2086],\n",
       "          [ 0.2770, -0.1903]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2619, -0.1627],\n",
       "          [ 0.2644, -0.1938],\n",
       "          [ 0.0000, -0.2223],\n",
       "          [ 0.2656, -0.1810]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2439, -0.1016],\n",
       "          [ 0.2553, -0.1810],\n",
       "          [ 0.2596, -0.0000],\n",
       "          [ 0.2638, -0.1992]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2434, -0.1946],\n",
       "          [ 0.2732, -0.1841],\n",
       "          [ 0.2664, -0.2078],\n",
       "          [ 0.0000, -0.2065]],\n",
       " \n",
       "         [[ 0.0000, -0.0306],\n",
       "          [ 0.2525, -0.1220],\n",
       "          [ 0.2648, -0.1445],\n",
       "          [ 0.2675, -0.1577],\n",
       "          [ 0.0000, -0.1900]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2478, -0.1580],\n",
       "          [ 0.2563, -0.2042],\n",
       "          [ 0.2633, -0.1970],\n",
       "          [ 0.2069, -0.1148]],\n",
       " \n",
       "         [[ 0.1971, -0.1267],\n",
       "          [ 0.2494, -0.1426],\n",
       "          [ 0.2070, -0.0000],\n",
       "          [ 0.2528, -0.0000],\n",
       "          [ 0.2754, -0.1784]],\n",
       " \n",
       "         [[ 0.0000, -0.1173],\n",
       "          [ 0.2490, -0.0000],\n",
       "          [ 0.2611, -0.1866],\n",
       "          [ 0.2597, -0.2190],\n",
       "          [ 0.2772, -0.1949]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2005, -0.0826],\n",
       "          [ 0.2476, -0.1648],\n",
       "          [ 0.2745, -0.1760],\n",
       "          [ 0.2807, -0.1937]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2006, -0.0715],\n",
       "          [ 0.2513, -0.1311],\n",
       "          [ 0.0000, -0.0887],\n",
       "          [ 0.2447, -0.1937]],\n",
       " \n",
       "         [[ 0.1874, -0.1564],\n",
       "          [ 0.2380, -0.2036],\n",
       "          [ 0.2593, -0.1731],\n",
       "          [ 0.2578, -0.2251],\n",
       "          [ 0.2771, -0.1975]],\n",
       " \n",
       "         [[ 0.1757, -0.0306],\n",
       "          [ 0.2333, -0.1524],\n",
       "          [ 0.2713, -0.1651],\n",
       "          [ 0.2666, -0.1952],\n",
       "          [ 0.2677, -0.1747]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2478, -0.1580],\n",
       "          [ 0.2072, -0.0944],\n",
       "          [ 0.2623, -0.1672],\n",
       "          [ 0.2782, -0.1880]],\n",
       " \n",
       "         [[ 0.1971, -0.0000],\n",
       "          [ 0.2402, -0.1929],\n",
       "          [ 0.2606, -0.1674],\n",
       "          [ 0.2645, -0.1730],\n",
       "          [ 0.2090, -0.1133]],\n",
       " \n",
       "         [[ 0.2056, -0.0976],\n",
       "          [ 0.2510, -0.1333],\n",
       "          [ 0.2636, -0.1517],\n",
       "          [ 0.2784, -0.1794],\n",
       "          [ 0.2654, -0.2266]],\n",
       " \n",
       "         [[ 0.2097, -0.1173],\n",
       "          [ 0.2434, -0.1946],\n",
       "          [ 0.2048, -0.0000],\n",
       "          [ 0.2610, -0.1765],\n",
       "          [ 0.2646, -0.2020]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmmodel(torch.randint(5, (64,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in flatten_model(model):\n",
    "    if has_params(m):\n",
    "        m.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples the hooks work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How working for layer: -- Conv2d --\n",
      "How working for layer: -- Linear --\n"
     ]
    }
   ],
   "source": [
    "class myNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(3,10,2, stride = 2)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.flatten = lambda x: x.view(-1)\n",
    "    self.fc1 = nn.Linear(160,5)\n",
    "   \n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.conv(x))\n",
    "    return self.fc1(self.flatten(x))\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "net.conv.register_forward_hook(hook_fn)\n",
    "net.fc1.register_forward_hook(hook_fn)\n",
    "inp = torch.randn(1,3,8,8)\n",
    "out = net(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(4688, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(4688, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=4688, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(4688, 400, padding_idx=1),\n",
       " Embedding(4688, 400, padding_idx=1),\n",
       " LSTM(400, 1152, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(1152, 1152, batch_first=True),\n",
       " ParameterModule(),\n",
       " LSTM(1152, 400, batch_first=True),\n",
       " ParameterModule(),\n",
       " Linear(in_features=400, out_features=4688, bias=True)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.activation_stats.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.rnn.LSTM"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modules[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.978067</td>\n",
       "      <td>5.422278</td>\n",
       "      <td>07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 61, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "torch.Size([61, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n",
      "Linear(in_features=400, out_features=4688, bias=True)\n",
      "------------Input Grad------------\n",
      "torch.Size([64, 72, 400])\n",
      "------------Output Grad------------\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "torch.Size([72, 4688])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-98423bb02982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# learner.fine_tune()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# learner.fit(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastcore/logargs.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/callback/schedule.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    111\u001b[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001b[1;32m    112\u001b[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastcore/logargs.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlog_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelValidException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/fastai/text/models/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdp_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp_inp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner.fit_one_cycle(1)\n",
    "# learner.fine_tune()\n",
    "# learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) [None,None,None,None,None,None,None,None,{'mean': -2.0935938358306885, 'std': 1.3332597017288208, 'near_zero': 0.931707408916159}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.cbs[5].stats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not plot the statistics of the weights for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.activation_stats.plot_layer_stats(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.activation_stats.color_dim(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this because we used a Callback/Hook named ActivationStats that keeps track of the statistics while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the statistics are just recorded for the last layer of the model. As we can see the first 8 layers just give the value None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(learn_tweets.activation_stats.stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) [None,None,None,None,None,None,None,None,{'mean': -1.8462823629379272, 'std': 1.2596983909606934, 'near_zero': 0.9194439246831189, 'hist': tensor([1.2411e+06, 1.0021e+06, 8.0230e+05, 6.4066e+05, 5.0985e+05, 4.0494e+05,\n",
       "        3.2018e+05, 2.5357e+05, 2.0129e+05, 1.5805e+05, 1.2631e+05, 1.0001e+05,\n",
       "        8.0982e+04, 6.4612e+04, 5.2268e+04, 4.2857e+04, 3.4609e+04, 2.8632e+04,\n",
       "        2.3751e+04, 1.9741e+04, 1.6459e+04, 1.3934e+04, 1.1901e+04, 1.0163e+04,\n",
       "        8.5460e+03, 7.2550e+03, 6.3900e+03, 5.4860e+03, 4.8030e+03, 4.0000e+03,\n",
       "        3.4060e+03, 2.9320e+03, 2.6210e+03, 2.2900e+03, 1.9420e+03, 1.6760e+03,\n",
       "        1.4160e+03, 1.2110e+03, 1.0560e+03, 8.4500e+02])}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.activation_stats.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_args': {'TextLearner.__init__.alpha': 2.0,\n",
       "  'TextLearner.__init__.beta': 1.0,\n",
       "  'TextLearner.__init__.moms': (0.8, 0.7, 0.8),\n",
       "  'TextLearner.__init__.loss_func': FlattenedLoss of CrossEntropyLoss(),\n",
       "  'TextLearner.__init__.lr': 0.001,\n",
       "  'TextLearner.__init__.splitter': <function fastai.text.models.awdlstm.awd_lstm_lm_split(model)>,\n",
       "  'TextLearner.__init__.metrics': None,\n",
       "  'TextLearner.__init__.path': Path('/Users/arnaujc/Projects/kaggle/data'),\n",
       "  'TextLearner.__init__.model_dir': 'models',\n",
       "  'TextLearner.__init__.wd': None,\n",
       "  'TextLearner.__init__.wd_bn_bias': False,\n",
       "  'TextLearner.__init__.train_bn': True,\n",
       "  'Learner.__init__.loss_func': FlattenedLoss of CrossEntropyLoss(),\n",
       "  'Learner.__init__.lr': 0.001,\n",
       "  'Learner.__init__.splitter': <function fastai.text.models.awdlstm.awd_lstm_lm_split(model)>,\n",
       "  'Learner.__init__.metrics': None,\n",
       "  'Learner.__init__.path': Path('/Users/arnaujc/Projects/kaggle/data'),\n",
       "  'Learner.__init__.model_dir': 'models',\n",
       "  'Learner.__init__.wd': None,\n",
       "  'Learner.__init__.wd_bn_bias': False,\n",
       "  'Learner.__init__.train_bn': True,\n",
       "  'Learner.__init__.moms': (0.8, 0.7, 0.8),\n",
       "  'language_model_learner.arch': fastai.text.models.awdlstm.AWD_LSTM,\n",
       "  'language_model_learner.config': None,\n",
       "  'language_model_learner.drop_mult': 1.0,\n",
       "  'language_model_learner.backwards': False,\n",
       "  'language_model_learner.pretrained': True,\n",
       "  'language_model_learner.pretrained_fnames': None,\n",
       "  'language_model_learner.loss_func': None,\n",
       "  'language_model_learner.lr': 0.001,\n",
       "  'language_model_learner.splitter': <function fastai.torch_core.trainable_params(m)>,\n",
       "  'language_model_learner.metrics': None,\n",
       "  'language_model_learner.path': Path('/Users/arnaujc/Projects/kaggle/data'),\n",
       "  'language_model_learner.model_dir': 'models',\n",
       "  'language_model_learner.wd': None,\n",
       "  'language_model_learner.wd_bn_bias': False,\n",
       "  'language_model_learner.train_bn': True,\n",
       "  'language_model_learner.moms': (0.95, 0.85, 0.95)},\n",
       " 'dls': <fastai.data.core.DataLoaders at 0x1a3f112f90>,\n",
       " 'model': SequentialRNN(\n",
       "   (0): AWD_LSTM(\n",
       "     (encoder): Embedding(264, 400, padding_idx=1)\n",
       "     (encoder_dp): EmbeddingDropout(\n",
       "       (emb): Embedding(264, 400, padding_idx=1)\n",
       "     )\n",
       "     (rnns): ModuleList(\n",
       "       (0): WeightDropout(\n",
       "         (module): LSTM(400, 1152, batch_first=True)\n",
       "       )\n",
       "       (1): WeightDropout(\n",
       "         (module): LSTM(1152, 1152, batch_first=True)\n",
       "       )\n",
       "       (2): WeightDropout(\n",
       "         (module): LSTM(1152, 400, batch_first=True)\n",
       "       )\n",
       "     )\n",
       "     (input_dp): RNNDropout()\n",
       "     (hidden_dps): ModuleList(\n",
       "       (0): RNNDropout()\n",
       "       (1): RNNDropout()\n",
       "       (2): RNNDropout()\n",
       "     )\n",
       "   )\n",
       "   (1): LinearDecoder(\n",
       "     (decoder): Linear(in_features=400, out_features=264, bias=True)\n",
       "     (output_dp): RNNDropout()\n",
       "   )\n",
       " ),\n",
       " '__stored_args__': {'dls': <fastai.data.core.DataLoaders at 0x1a3f112f90>,\n",
       "  'model': SequentialRNN(\n",
       "    (0): AWD_LSTM(\n",
       "      (encoder): Embedding(264, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(264, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "    (1): LinearDecoder(\n",
       "      (decoder): Linear(in_features=400, out_features=264, bias=True)\n",
       "      (output_dp): RNNDropout()\n",
       "    )\n",
       "  ),\n",
       "  'loss_func': FlattenedLoss of CrossEntropyLoss(),\n",
       "  'opt_func': <function fastai.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)>,\n",
       "  'lr': 0.001,\n",
       "  'splitter': <function fastai.text.models.awdlstm.awd_lstm_lm_split(model)>,\n",
       "  'cbs': None,\n",
       "  'metrics': None,\n",
       "  'path': Path('/Users/arnaujc/Projects/kaggle/data'),\n",
       "  'model_dir': 'models',\n",
       "  'wd': None,\n",
       "  'wd_bn_bias': False,\n",
       "  'train_bn': True,\n",
       "  'moms': (0.8, 0.7, 0.8)},\n",
       " 'loss_func': FlattenedLoss of CrossEntropyLoss(),\n",
       " 'opt_func': <function fastai.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)>,\n",
       " 'lr': 0.001,\n",
       " 'splitter': <function fastai.text.models.awdlstm.awd_lstm_lm_split(model)>,\n",
       " 'cbs': (#5) [TrainEvalCallback,Recorder,ProgressCallback,ModelResetter,RNNRegularizer],\n",
       " '_metrics': (#0) [],\n",
       " 'path': Path('/Users/arnaujc/Projects/kaggle/data'),\n",
       " 'model_dir': 'models',\n",
       " 'wd': None,\n",
       " 'wd_bn_bias': False,\n",
       " 'train_bn': True,\n",
       " 'moms': (0.8, 0.7, 0.8),\n",
       " 'training': False,\n",
       " 'create_mbar': True,\n",
       " 'logger': <function print>,\n",
       " 'opt': <fastai.optimizer.Optimizer at 0x1a42c87ad0>,\n",
       " 'train_eval': TrainEvalCallback,\n",
       " 'recorder': Recorder,\n",
       " 'progress': ProgressCallback,\n",
       " 'epoch': 0,\n",
       " 'n_epoch': 1,\n",
       " 'loss': tensor(0.),\n",
       " 'model_resetter': ModelResetter,\n",
       " 'rnn_regularizer': RNNRegularizer}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 6.0867e-01, -5.2552e-01,  1.5330e-01,  ..., -2.5346e-01,\n",
       "         -3.3278e-01, -6.8334e-02],\n",
       "        [-4.5477e-01,  8.9764e-02, -8.1741e-02,  ..., -1.9039e-03,\n",
       "          4.9093e-01,  4.6553e-02],\n",
       "        [-1.1224e-01, -3.2417e-01,  1.5993e-04,  ...,  5.1675e-02,\n",
       "         -1.7925e-01,  4.2092e-01],\n",
       "        ...,\n",
       "        [ 2.5333e-02,  2.5912e-03,  4.2268e-02,  ...,  1.2799e-02,\n",
       "          8.4781e-02, -1.8249e-03],\n",
       "        [ 2.5333e-02,  2.5912e-03,  4.2268e-02,  ...,  1.2799e-02,\n",
       "          8.4781e-02, -1.8249e-03],\n",
       "        [ 2.5333e-02,  2.5912e-03,  4.2268e-02,  ...,  1.2799e-02,\n",
       "          8.4781e-02, -1.8249e-03]], requires_grad=True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model[0].encoder.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
